{"version":3,"file":"vendors-node_modules_jbrowse_plugin-alignments_esm_BamAdapter_BamAdapter_js.bundle.js","mappings":";;;;;;;;;;;;;;;;;;;AAAwB;AACoB;AAChB;AACQ;AACkB;AACtD,4BAA4B;AAC5B;AACA;AACA;AACA;AACA;AACA;AACe,kBAAkB,kDAAS;AAC1C;AACA,0BAA0B,mDAAY,CAAC,uDAAgB;AACvD,iBAAiB;AACjB;AACA,oCAAoC;AACpC;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,sBAAsB;AACtB;AACA;AACA;AACA;AACA,aAAa;AACb;AACA;AACA;AACA;AACA;AACA,uBAAuB;AACvB;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,wBAAwB,mBAAmB;AAC3C;AACA;AACA;AACA;AACA;AACA,4BAA4B,cAAc;AAC1C;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,oCAAoC,gBAAgB;AACpD,kCAAkC,yDAAS;AAC3C,kCAAkC,yDAAS;AAC3C;AACA;AACA,wCAAwC,8CAAK;AAC7C;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,4BAA4B,iBAAiB;AAC7C,iCAAiC,yDAAS;AAC1C;AACA;AACA;AACA,gCAAgC;AAChC;AACA;AACA;AACA,+CAA+C;AAC/C;AACA;AACA;AACA;AACA;AACA;AACA;AACA,gBAAgB,0BAA0B;AAC1C;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,mCAAmC,WAAW;AAC9C;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,qBAAqB;AACrB,SAAS;AACT;AACA;AACA;AACA,iBAAiB;AACjB;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,mDAAmD;AACnD;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,kCAAkC,YAAY;AAC9C;AACA;AACA,oCAAoC,sBAAsB;AAC1D,wCAAwC,8CAAK;AAC7C;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,6BAA6B,aAAa;AAC1C;AACA;AACA;AACA;AACA;AACA;AACA;AACA,eAAe,qDAAc;AAC7B;AACA;AACA;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;AC/LiC;AAC8B;AACjB;AACa;AACC;AAC3B;AACjC;AACwB;AACA;AACU;AACM;AACuC;AACxE;AACP;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACe;AACf;AACA,eAAe,QAAQ;AACvB,eAAe,QAAQ;AACvB,eAAe,YAAY;AAC3B,eAAe,QAAQ;AACvB,eAAe,YAAY;AAC3B;AACA,kBAAkB,gLAAgL;AAClM,gCAAgC,gEAAqB;AACrD;AACA,uBAAuB,kDAAQ;AAC/B;AACA,aAAa;AACb;AACA,2BAA2B,aAAa;AACxC,wBAAwB,+BAA+B;AACvD;AACA,4BAA4B,iBAAiB;AAC7C,iBAAiB;AACjB;AACA;AACA,aAAa;AACb,SAAS;AACT;AACA;AACA;AACA;AACA;AACA,2BAA2B,yDAAS;AACpC;AACA;AACA,2BAA2B,0DAAU;AACrC;AACA;AACA;AACA;AACA;AACA,6BAA6B,4CAAG,GAAG,2BAA2B;AAC9D;AACA;AACA,6BAA6B,4CAAG,GAAG,gBAAgB,yDAAS,WAAW;AACvE;AACA;AACA,6BAA6B,4CAAG,GAAG,gBAAgB,0DAAU,UAAU;AACvE;AACA;AACA,6BAA6B,4CAAG,GAAG,2BAA2B;AAC9D;AACA;AACA,6BAA6B,4CAAG,GAAG,gBAAgB,yDAAS,WAAW;AACvE;AACA;AACA,6BAA6B,4CAAG,GAAG,gBAAgB,0DAAU,UAAU;AACvE;AACA;AACA,6BAA6B,4CAAG,GAAG,gBAAgB,yDAAS,IAAI,QAAQ,QAAQ;AAChF;AACA;AACA,6BAA6B,4CAAG,GAAG,gBAAgB,0DAAU,IAAI,OAAO,QAAQ;AAChF;AACA;AACA;AACA;AACA,2DAA2D;AAC3D,2DAA2D;AAC3D;AACA;AACA,iCAAiC;AACjC,qBAAqB,gDAAQ;AAC7B;AACA;AACA;AACA;AACA;AACA;AACA,4CAA4C,MAAM;AAClD,oBAAoB,YAAY;AAChC,eAAe,SAAS;AACxB;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,4BAA4B,4DAAK;AACjC;AACA;AACA;AACA;AACA;AACA,gBAAgB,yBAAyB;AACzC;AACA;AACA,eAAe,qDAAe;AAC9B;AACA,iCAAiC;AACjC;AACA;AACA;AACA;AACA;AACA,oDAAoD;AACpD;AACA;AACA;AACA;AACA,gBAAgB,oBAAoB,sBAAsB,MAAM;AAChE;AACA;AACA;AACA,4BAA4B,4DAAK;AACjC;AACA;AACA;AACA;AACA,wBAAwB,UAAU;AAClC;AACA;AACA;AACA;AACA,8BAA8B,uBAAuB;AACrD;AACA;AACA,qEAAqE,aAAa;AAClF;AACA;AACA;AACA,iBAAiB;AACjB;AACA;AACA;AACA;AACA;AACA,KAAK;AACL;AACA;AACA,yDAAyD;AACzD,gBAAgB,SAAS;AACzB;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,wBAAwB,mBAAmB;AAC3C,kBAAkB,uDAAe;AACjC;AACA;AACA,yEAAyE,MAAM,kCAAkC,oBAAoB;AACrI;AACA;AACA;AACA;AACA;AACA;AACA,4CAA4C,4BAA4B,qCAAqC,sCAAsC;AACnJ;AACA;AACA;AACA;AACA,gBAAgB,sBAAsB;AACtC;AACA;AACA,wBAAwB,mBAAmB;AAC3C;AACA;AACA;AACA;AACA,aAAa;AACb;AACA,4BAA4B,oBAAoB;AAChD;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,QAAQ,wDAAgB;AACxB;AACA;AACA;AACA;AACA;AACA,gBAAgB,gDAAgD;AAChE;AACA;AACA;AACA;AACA,4BAA4B,gBAAgB;AAC5C;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,YAAY,8DAAO;AACnB;AACA;AACA;AACA,aAAa;AACb,SAAS;AACT;AACA;AACA,4BAA4B,gBAAgB;AAC5C;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,SAAS;AACT;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,4CAA4C,gCAAgC,qCAAqC,sCAAsC;AACvJ;AACA;AACA,oBAAoB,sCAAsC;AAC1D;AACA;AACA,aAAa;AACb;AACA;AACA,4BAA4B,kBAAkB;AAC9C;AACA;AACA;AACA;AACA;AACA;AACA,SAAS;AACT;AACA;AACA,uBAAuB,aAAa;AACpC;AACA,gBAAgB,oBAAoB,sBAAsB,MAAM;AAChE,gBAAgB,wCAAwC,QAAQ,sEAAe;AAC/E,iBAAiB;AACjB;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,oCAAoC,+CAAU;AAC9C;AACA;AACA;AACA;AACA,qBAAqB;AACrB;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,4BAA4B,0DAAY;AACxC,iBAAiB;AACjB;AACA;AACA,0BAA0B,+CAAO;AACjC;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;;;;;;;;;;;;;;ACxXA;AACe;AACf;AACA,eAAe,eAAe;AAC9B,eAAe,eAAe;AAC9B,eAAe,QAAQ;AACvB,eAAe,QAAQ;AACvB;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,kBAAkB,UAAU,IAAI,WAAW,OAAO,SAAS,gBAAgB,mBAAmB;AAC9F;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;;;;;;;;;;;AChCA,+DAAe;AACf;AACA;AACA;AACA;AACA,qCAAqC;AACrC;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,CAAC,EAAC;AACF;;;;;;;;;;;;;;;;;;;;;;AC1BwB;AACsB;AACa;AAC/B;AAC2C;AACnC;AACpC,6BAA6B;AAC7B,6BAA6B;AAC7B;AACA;AACA;AACA;AACA;AACA;AACe,kBAAkB,kDAAS;AAC1C;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,gBAAgB,QAAQ;AACxB;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,wBAAwB,kCAAkC;AAC1D;AACA,iEAAiE,iBAAiB;AAClF;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,wBAAwB,uBAAuB;AAC/C;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,iBAAiB;AACjB;AACA;AACA;AACA,uBAAuB;AACvB;AACA,4BAA4B,4DAAK;AACjC;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,wBAAwB,mBAAmB;AAC3C,kBAAkB,sDAAe;AACjC;AACA;AACA;AACA;AACA,uBAAuB;AACvB,4BAA4B,cAAc;AAC1C;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,oCAAoC,yDAAS;AAC7C;AACA;AACA;AACA;AACA,oCAAoC,gBAAgB;AACpD,kCAAkC,yDAAS;AAC3C,kCAAkC,yDAAS;AAC3C;AACA;AACA,wCAAwC,8CAAK;AAC7C;AACA;AACA;AACA;AACA,gCAAgC;AAChC;AACA;AACA;AACA;AACA,0BAA0B,mDAAY,CAAC,uDAAgB;AACvD,iBAAiB;AACjB;AACA,mDAAmD;AACnD;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,yDAAyD;AACzD;AACA;AACA;AACA,kCAAkC,YAAY;AAC9C;AACA;AACA,oCAAoC,sBAAsB;AAC1D,wCAAwC,8CAAK;AAC7C;AACA;AACA;AACA;AACA,eAAe,qDAAc,aAAa,sDAAa;AACvD;AACA;AACA;AACA,iBAAiB;AACjB;AACA;AACA,kBAAkB;AAClB;AACA;AACA;AACA;AACA;AACA,UAAU;AACV;AACA;AACA;AACA;AACA;AACA,eAAe,iBAAiB;AAChC;AACA;AACA;AACA,yCAAyC,IAAI,GAAG,KAAK,iDAAiD,cAAc,UAAU,WAAW;AACzI;AACA;AACA;AACA;AACA;AACA;AACA;;;;;;;;;;;;;;;;;;;;;AC3M+C;AACjB;AACgB;AACN;AACxC;AACA;AACA,gBAAgB,eAAe;AAC/B;AACA,mBAAmB,MAAM;AACzB;AACA;AACA;AACA;AACA;AACA,oBAAoB,mBAAmB;AACvC;AACA;AACA,2BAA2B,0BAA0B;AACrD,aAAa;AACb;AACA,mDAAmD,eAAe;AAClE;AACA,mBAAmB,MAAM;AACzB;AACA,KAAK;AACL,WAAW,MAAM,yCAAyC,4DAAK;AAC/D;AACe,yBAAyB,gDAAO;AAC/C;AACA;AACA,gBAAgB,wCAAwC;AACxD;AACA;AACA;AACA;AACA;AACA;AACA;AACA,KAAK;AACL,wBAAwB,aAAa,GAAG,aAAa;AACrD,uBAAuB,KAAK,iBAAiB,IAAI,SAAS,IAAI,OAAO,IAAI;AACzE;AACA,0CAA0C,SAAS;AACnD;AACA;AACA;AACA;AACA;AACA;AACA;AACA,qBAAqB,QAAQ,mBAAmB;AAChD;AACA,0BAA0B,IAAI,GAAG,IAAI,GAAG,IAAI;AAC5C,aAAa;AACb;AACA;AACA;AACA;AACA;AACA;AACA;AACA,gBAAgB,QAAQ;AACxB,gBAAgB,oBAAoB;AACpC,iBAAiB;AACjB;AACA,6BAA6B;AAC7B,uBAAuB,aAAa,GAAG,aAAa;AACpD;AACA;AACA,+CAA+C,kBAAkB;AACjE;AACA;AACA;AACA,qCAAqC,+CAAS;AAC9C;AACA;AACA;AACA;AACA,0BAA0B,qDAAe;AACzC;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,aAAa;AACb,SAAS;AACT;AACA;AACA;AACA;AACA;AACA;;;;;;;;;;;;;;;;;;;;;;;;ACnGwB;AACA;AACQ;AACE;AACD;AACmB;AACpD;;;;;;;;;;;;;;;ACNe;AACf;AACA,eAAe,YAAY;AAC3B,eAAe,UAAU;AACzB;AACA,kBAAkB,sCAAsC;AACxD;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,yBAAyB;AACzB;AACA;AACA;AACA;AACA,aAAa;AACb;AACA;AACA;AACA,oCAAoC;AACpC,+DAA+D;AAC/D;AACA;AACA;;;;;;;;;;;;;;;;AChCA;AACoC;AACpC;AACA;AACA;AACA;AACA;AACe;AACf;AACA;AACA;AACA;AACA,gBAAgB,oBAAoB;AACpC,gBAAgB,mBAAmB;AACnC;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,SAAS;AACT;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,SAAS;AACT;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,gBAAgB,mBAAmB;AACnC;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,gBAAgB,mBAAmB;AACnC;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,gBAAgB,mBAAmB;AACnC;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,4CAA4C,WAAW;AACvD;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,4CAA4C,WAAW;AACvD;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,4CAA4C,WAAW;AACvD;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,4CAA4C,WAAW;AACvD;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,wCAAwC,WAAW;AACnD;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,wCAAwC,WAAW;AACnD;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,wCAAwC,WAAW;AACnD;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,wCAAwC,WAAW;AACnD;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,wCAAwC,WAAW;AACnD;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,0DAA0D,KAAK;AAC/D;AACA,kCAAkC;AAClC;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,iBAAiB,SAAS;AAC1B;AACA;AACA,+BAA+B,8DAAqB;AACpD;AACA,kBAAkB,SAAS;AAC3B;AACA,+BAA+B,mEAA0B;AACzD;AACA,kBAAkB,SAAS,qCAAqC;AAChE;AACA,+BAA+B,6DAAoB;AACnD;AACA,kBAAkB,SAAS,qCAAqC;AAChE;AACA,+BAA+B,8DAAqB;AACpD;AACA,kBAAkB,SAAS;AAC3B;AACA,+BAA+B,+DAAsB;AACrD;AACA,kBAAkB,SAAS;AAC3B;AACA,+BAA+B,gEAAuB;AACtD;AACA,kBAAkB,SAAS;AAC3B;AACA,+BAA+B,6DAAoB;AACnD;AACA,kBAAkB,SAAS;AAC3B;AACA,+BAA+B,6DAAoB;AACnD;AACA,kBAAkB,SAAS;AAC3B;AACA,+BAA+B,iEAAwB;AACvD;AACA,kBAAkB,SAAS;AAC3B;AACA,+BAA+B,8DAAqB;AACpD;AACA,kBAAkB,SAAS;AAC3B;AACA,+BAA+B,2DAAkB;AACjD;AACA,kBAAkB,SAAS;AAC3B;AACA,+BAA+B,qEAA4B;AAC3D;AACA;AACA;AACA;AACA;AACA,gBAAgB,mBAAmB;AACnC;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,4BAA4B,iBAAiB;AAC7C;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,+BAA+B;AAC/B;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,gBAAgB,mBAAmB;AACnC;AACA;AACA;AACA;AACA;AACA,wBAAwB,cAAc;AACtC;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,SAAS;AACT;AACA;AACA;AACA;;;;;;;;;;;;;;;AC9gBO;AACP;AACA;AACA;AACA;AACA;AACA;AACA,qBAAqB;AACrB,SAAS;AACT;AACA,wBAAwB,wCAAwC;AAChE;AACA,KAAK;AACL;AACA;AACA;;;;;;;;;;;;;;;;;;;;;ACfO;AACP;AACA;AACO;AACP;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,WAAW,aAAa;AACxB;AACA;AACO;AACP;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,WAAW,aAAa;AACxB;AACO;AACP;AACA;AACA;AACO;AACP;AACA;AACA;AACO,0BAA0B;AACjC,gCAAgC,cAAc;AAC9C;AACO;AACP;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,KAAK;AACL;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,KAAK;AACL;AACA;AACA;;;;;;;;;;;;;;;;AC3Fe;AACf;AACA,4CAA4C;AAC5C,0CAA0C;AAC1C;AACA;AACA,kBAAkB,mBAAmB,GAAG,kBAAkB;AAC1D;AACA;AACA;AACA;AACA;AACA;AACA;AACA,eAAe,MAAM;AACrB;AACA;AACA,eAAe,iBAAiB;AAChC;AACA;AACA;AACA;AACA;AACA;AACA;AACO;AACP;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;;;;;;;;;;;;;;;;;;;;;;;;;;ACpCoC;AAC8C;AACf;AACd;AACM;AAClB;AACoB;AACC;AAC/C,yBAAyB,2FAAsB;AAC9D;AACA;AACA;AACA;AACA,gCAAgC,2EAAc;AAC9C,6BAA6B,2EAAc;AAC3C,8BAA8B,2EAAc;AAC5C,4BAA4B,8CAAO;AACnC,+BAA+B,mEAAY;AAC3C;AACA,sBAAsB,mEAAY;AAClC;AACA;AACA,sBAAsB,mEAAY;AAClC;AACA;AACA;AACA;AACA;AACA;AACA;AACA,aAAa;AACb,kCAAkC,2EAAc;AAChD;AACA,4EAA4E,aAAa;AACzF;AACA;AACA,iBAAiB;AACjB;AACA;AACA,oDAAoD,KAAK;AACzD;AACA;AACA;AACA;AACA;AACA,gBAAgB,MAAM;AACtB;AACA;AACA;AACA,gBAAgB,6BAA6B;AAC7C,gBAAgB,MAAM;AACtB,+BAA+B,gEAAY;AAC3C;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,iBAAiB;AACjB,aAAa;AACb,qBAAqB;AACrB,SAAS;AACT;AACA;AACA;AACA;AACA;AACA;AACA;AACA,aAAa;AACb;AACA;AACA;AACA;AACA,gBAAgB,WAAW;AAC3B;AACA;AACA;AACA,gBAAgB,kBAAkB;AAClC;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,SAAS;AACT,8CAA8C,uDAAO;AACrD;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,SAAS;AACT;AACA,+DAA+D,QAAQ,GAAG,6BAA6B,GAAG,sBAAsB,WAAW,kCAAkC,kCAAkC,+BAA+B;AAC9O;AACA;AACA;AACA;AACA,gBAAgB,uCAAuC;AACvD,gBAAgB,+CAA+C;AAC/D,eAAe,yEAAgB;AAC/B,oBAAoB,MAAM;AAC1B;AACA;AACA;AACA,oBAAoB,yDAAyD;AAC7E;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,kCAAkC,+DAAsB;AACxD;AACA;AACA;AACA,SAAS;AACT;AACA;AACA,gBAAgB,MAAM;AACtB;AACA;AACA;AACA,gCAAgC,mEAAe;AAC/C,mCAAmC,2EAAc;AACjD,qBAAqB;AACrB;AACA;AACA;AACA;AACA;AACA,wBAAwB,SAAS;AACjC;AACA;AACA;AACA;AACA;AACA;AACA;;;;;;;;;;;;;;;;AC5KiD;AAClC;AACf;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,gBAAgB,kBAAkB;AAClC;AACA,iBAAiB,0CAA0C,GAAG,uBAAuB;AACrF;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,kBAAkB,gBAAgB,GAAG,iBAAiB;AACtD;AACA;AACA;AACA,mCAAmC,MAAM;AACzC;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,eAAe,8DAAa;AAC5B;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;;;;;;;;;AC1GA;;AAEA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,UAAU;AACV;AACA;AACA,OAAO;AACP;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA,gBAAgB;AAChB;AACA;;AAEA;AACA;AACA;AACA;AACA;;AAEA;AACA;;AAEA;AACA;;AAEA;AACA;AACA;AACA,OAAO;AACP,MAAM;AACN;AACA;AACA,OAAO;AACP,MAAM;AACN;AACA;AACA,OAAO;AACP;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA,KAAK;AACL;AACA;;AAEA;AACA;AACA;AACA;AACA,KAAK;AACL;AACA;;AAEA;AACA;AACA;AACA;AACA,KAAK;AACL;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,KAAK;AACL;;AAEA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;;AAEA,oBAAoB,iBAAiB;AACrC;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA,MAAM;AACN;AACA;AACA;AACA;AACA;;AAEA;AACA;;AAEA;AACA;AACA;AACA;AACA,QAAQ;AACR;AACA,QAAQ;AACR;AACA,QAAQ;AACR;AACA,QAAQ;AACR;AACA,QAAQ;AACR;AACA;AACA;AACA,QAAQ;AACR;AACA,QAAQ;AACR;AACA;;AAEA;AACA;AACA,uDAAuD;AACvD,UAAU;AACV;AACA,UAAU;AACV,8EAA8E;AAC9E;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA,UAAU;AACV;AACA,UAAU;AACV;AACA,UAAU;AACV;AACA;AACA;;AAEA;AACA;AACA;AACA,UAAU;AACV;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA,QAAQ;AACR;AACA,QAAQ;AACR;AACA,QAAQ;AACR;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;;AAEA;AACA;;AAEA;AACA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,MAAM;AACN;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;;AAEA;AACA,8BAA8B,qBAAqB;AACnD;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,OAAO;AACP;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,KAAK;AACL;AACA;;AAEA;;AAEA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA,KAAK;AACL;;AAEA;AACA,uCAAuC,0BAA0B;AACjE;AACA;AACA;;AAEA;;AAEA;AACA;AACA;AACA;;AAEA,+BAA+B,0BAA0B,eAAe;AACxE;;AAEA;AACA;AACA;AACA,IAAI;AACJ;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;;AAEA;AACA;AACA,QAAQ;AACR;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA,OAAO;;AAEP;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA,KAAK;AACL;;AAEA;;AAEA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;;AAEA,iDAAiD,aAAa;;AAE9D;;AAEA,CAAC,IAAI;AACL,CAAC","sources":["webpack://taxonium/./node_modules/@gmod/bam/esm/bai.js","webpack://taxonium/./node_modules/@gmod/bam/esm/bamFile.js","webpack://taxonium/./node_modules/@gmod/bam/esm/chunk.js","webpack://taxonium/./node_modules/@gmod/bam/esm/constants.js","webpack://taxonium/./node_modules/@gmod/bam/esm/csi.js","webpack://taxonium/./node_modules/@gmod/bam/esm/htsget.js","webpack://taxonium/./node_modules/@gmod/bam/esm/index.js","webpack://taxonium/./node_modules/@gmod/bam/esm/indexFile.js","webpack://taxonium/./node_modules/@gmod/bam/esm/record.js","webpack://taxonium/./node_modules/@gmod/bam/esm/sam.js","webpack://taxonium/./node_modules/@gmod/bam/esm/util.js","webpack://taxonium/./node_modules/@gmod/bam/esm/virtualOffset.js","webpack://taxonium/./node_modules/@jbrowse/plugin-alignments/esm/BamAdapter/BamAdapter.js","webpack://taxonium/./node_modules/@jbrowse/plugin-alignments/esm/BamAdapter/BamSlightlyLazyFeature.js","webpack://taxonium/./node_modules/cross-fetch/dist/browser-polyfill.js"],"sourcesContent":["import Long from 'long';\nimport { fromBytes } from './virtualOffset';\nimport Chunk from './chunk';\nimport IndexFile from './indexFile';\nimport { longToNumber, optimizeChunks } from './util';\nconst BAI_MAGIC = 21578050; // BAI\\1\nfunction roundDown(n, multiple) {\n    return n - (n % multiple);\n}\nfunction roundUp(n, multiple) {\n    return n - (n % multiple) + multiple;\n}\nexport default class BAI extends IndexFile {\n    parsePseudoBin(bytes, offset) {\n        const lineCount = longToNumber(Long.fromBytesLE(Array.prototype.slice.call(bytes, offset + 16, offset + 24), true));\n        return { lineCount };\n    }\n    async lineCount(refId, opts = {}) {\n        const prom = await this.parse(opts);\n        const index = prom.indices[refId];\n        if (!index) {\n            return -1;\n        }\n        const ret = index.stats || {};\n        return ret.lineCount === undefined ? -1 : ret.lineCount;\n    }\n    fetchBai(opts = {}) {\n        if (!this.baiP) {\n            this.baiP = this.filehandle.readFile(opts).catch(e => {\n                this.baiP = undefined;\n                throw e;\n            });\n        }\n        return this.baiP;\n    }\n    // fetch and parse the index\n    async _parse() {\n        const data = { bai: true, maxBlockSize: 1 << 16 };\n        const bytes = await this.fetchBai();\n        // check BAI magic numbers\n        if (bytes.readUInt32LE(0) !== BAI_MAGIC) {\n            throw new Error('Not a BAI file');\n        }\n        data.refCount = bytes.readInt32LE(4);\n        const depth = 5;\n        const binLimit = ((1 << ((depth + 1) * 3)) - 1) / 7;\n        // read the indexes for each reference sequence\n        data.indices = new Array(data.refCount);\n        let currOffset = 8;\n        for (let i = 0; i < data.refCount; i += 1) {\n            // the binning index\n            const binCount = bytes.readInt32LE(currOffset);\n            let stats;\n            currOffset += 4;\n            const binIndex = {};\n            for (let j = 0; j < binCount; j += 1) {\n                const bin = bytes.readUInt32LE(currOffset);\n                currOffset += 4;\n                if (bin === binLimit + 1) {\n                    currOffset += 4;\n                    stats = this.parsePseudoBin(bytes, currOffset);\n                    currOffset += 32;\n                }\n                else if (bin > binLimit + 1) {\n                    throw new Error('bai index contains too many bins, please use CSI');\n                }\n                else {\n                    const chunkCount = bytes.readInt32LE(currOffset);\n                    currOffset += 4;\n                    const chunks = new Array(chunkCount);\n                    for (let k = 0; k < chunkCount; k += 1) {\n                        const u = fromBytes(bytes, currOffset);\n                        const v = fromBytes(bytes, currOffset + 8);\n                        currOffset += 16;\n                        this._findFirstData(data, u);\n                        chunks[k] = new Chunk(u, v, bin);\n                    }\n                    binIndex[bin] = chunks;\n                }\n            }\n            const linearCount = bytes.readInt32LE(currOffset);\n            currOffset += 4;\n            // as we're going through the linear index, figure out\n            // the smallest virtual offset in the indexes, which\n            // tells us where the BAM header ends\n            const linearIndex = new Array(linearCount);\n            for (let k = 0; k < linearCount; k += 1) {\n                linearIndex[k] = fromBytes(bytes, currOffset);\n                currOffset += 8;\n                this._findFirstData(data, linearIndex[k]);\n            }\n            data.indices[i] = { binIndex, linearIndex, stats };\n        }\n        return data;\n    }\n    async indexCov(seqId, start, end, opts = {}) {\n        const v = 16384;\n        const range = start !== undefined;\n        const indexData = await this.parse(opts);\n        const seqIdx = indexData.indices[seqId];\n        if (!seqIdx) {\n            return [];\n        }\n        const { linearIndex = [], stats } = seqIdx;\n        if (!linearIndex.length) {\n            return [];\n        }\n        const e = end !== undefined ? roundUp(end, v) : (linearIndex.length - 1) * v;\n        const s = start !== undefined ? roundDown(start, v) : 0;\n        let depths;\n        if (range) {\n            depths = new Array((e - s) / v);\n        }\n        else {\n            depths = new Array(linearIndex.length - 1);\n        }\n        const totalSize = linearIndex[linearIndex.length - 1].blockPosition;\n        if (e > (linearIndex.length - 1) * v) {\n            throw new Error('query outside of range of linear index');\n        }\n        let currentPos = linearIndex[s / v].blockPosition;\n        for (let i = s / v, j = 0; i < e / v; i++, j++) {\n            depths[j] = {\n                score: linearIndex[i + 1].blockPosition - currentPos,\n                start: i * v,\n                end: i * v + v,\n            };\n            currentPos = linearIndex[i + 1].blockPosition;\n        }\n        return depths.map(d => {\n            return { ...d, score: (d.score * stats.lineCount) / totalSize };\n        });\n    }\n    /**\n     * calculate the list of bins that may overlap with region [beg,end) (zero-based half-open)\n     * @returns {Array[number]}\n     */\n    reg2bins(beg, end) {\n        end -= 1;\n        return [\n            [0, 0],\n            [1 + (beg >> 26), 1 + (end >> 26)],\n            [9 + (beg >> 23), 9 + (end >> 23)],\n            [73 + (beg >> 20), 73 + (end >> 20)],\n            [585 + (beg >> 17), 585 + (end >> 17)],\n            [4681 + (beg >> 14), 4681 + (end >> 14)],\n        ];\n    }\n    async blocksForRange(refId, min, max, opts = {}) {\n        if (min < 0) {\n            min = 0;\n        }\n        const indexData = await this.parse(opts);\n        if (!indexData) {\n            return [];\n        }\n        const ba = indexData.indices[refId];\n        if (!ba) {\n            return [];\n        }\n        // List of bin #s that overlap min, max\n        const overlappingBins = this.reg2bins(min, max);\n        const chunks = [];\n        // Find chunks in overlapping bins.  Leaf bins (< 4681) are not pruned\n        for (const [start, end] of overlappingBins) {\n            for (let bin = start; bin <= end; bin++) {\n                if (ba.binIndex[bin]) {\n                    const binChunks = ba.binIndex[bin];\n                    for (let c = 0; c < binChunks.length; ++c) {\n                        chunks.push(new Chunk(binChunks[c].minv, binChunks[c].maxv, bin));\n                    }\n                }\n            }\n        }\n        // Use the linear index to find minimum file position of chunks that could\n        // contain alignments in the region\n        const nintv = ba.linearIndex.length;\n        let lowest = null;\n        const minLin = Math.min(min >> 14, nintv - 1);\n        const maxLin = Math.min(max >> 14, nintv - 1);\n        for (let i = minLin; i <= maxLin; ++i) {\n            const vp = ba.linearIndex[i];\n            if (vp) {\n                if (!lowest || vp.compareTo(lowest) < 0) {\n                    lowest = vp;\n                }\n            }\n        }\n        return optimizeChunks(chunks, lowest);\n    }\n}\n//# sourceMappingURL=bai.js.map","import crc32 from 'buffer-crc32';\nimport { unzip, unzipChunkSlice } from '@gmod/bgzf-filehandle';\nimport entries from 'object.entries-ponyfill';\nimport { LocalFile, RemoteFile } from 'generic-filehandle';\nimport AbortablePromiseCache from 'abortable-promise-cache';\nimport QuickLRU from 'quick-lru';\n//locals\nimport BAI from './bai';\nimport CSI from './csi';\nimport BAMFeature from './record';\nimport { parseHeaderText } from './sam';\nimport { abortBreakPoint, checkAbortSignal, timeout, makeOpts, } from './util';\nexport const BAM_MAGIC = 21840194;\nconst blockLen = 1 << 16;\nfunction flat(arr) {\n    return [].concat(...arr);\n}\nasync function gen2array(gen) {\n    const out = [];\n    for await (const x of gen) {\n        out.push(x);\n    }\n    return out;\n}\nexport default class BamFile {\n    /**\n     * @param {object} args\n     * @param {string} [args.bamPath]\n     * @param {FileHandle} [args.bamFilehandle]\n     * @param {string} [args.baiPath]\n     * @param {FileHandle} [args.baiFilehandle]\n     */\n    constructor({ bamFilehandle, bamPath, bamUrl, baiPath, baiFilehandle, baiUrl, csiPath, csiFilehandle, csiUrl, fetchSizeLimit, chunkSizeLimit, yieldThreadTime = 100, renameRefSeqs = n => n, }) {\n        this.featureCache = new AbortablePromiseCache({\n            //@ts-ignore\n            cache: new QuickLRU({\n                maxSize: 50,\n            }),\n            //@ts-ignore\n            fill: async ({ chunk, opts }, signal) => {\n                const { data, cpositions, dpositions } = await this._readChunk({\n                    chunk,\n                    opts: { ...opts, signal },\n                });\n                const feats = await this.readBamFeatures(data, cpositions, dpositions, chunk);\n                return feats;\n            },\n        });\n        this.renameRefSeq = renameRefSeqs;\n        if (bamFilehandle) {\n            this.bam = bamFilehandle;\n        }\n        else if (bamPath) {\n            this.bam = new LocalFile(bamPath);\n        }\n        else if (bamUrl) {\n            this.bam = new RemoteFile(bamUrl);\n        }\n        else {\n            throw new Error('unable to initialize bam');\n        }\n        if (csiFilehandle) {\n            this.index = new CSI({ filehandle: csiFilehandle });\n        }\n        else if (csiPath) {\n            this.index = new CSI({ filehandle: new LocalFile(csiPath) });\n        }\n        else if (csiUrl) {\n            this.index = new CSI({ filehandle: new RemoteFile(csiUrl) });\n        }\n        else if (baiFilehandle) {\n            this.index = new BAI({ filehandle: baiFilehandle });\n        }\n        else if (baiPath) {\n            this.index = new BAI({ filehandle: new LocalFile(baiPath) });\n        }\n        else if (baiUrl) {\n            this.index = new BAI({ filehandle: new RemoteFile(baiUrl) });\n        }\n        else if (bamPath) {\n            this.index = new BAI({ filehandle: new LocalFile(`${bamPath}.bai`) });\n        }\n        else if (bamUrl) {\n            this.index = new BAI({ filehandle: new RemoteFile(`${bamUrl}.bai`) });\n        }\n        else {\n            throw new Error('unable to infer index format');\n        }\n        this.fetchSizeLimit = fetchSizeLimit || 500000000; // 500MB\n        this.chunkSizeLimit = chunkSizeLimit || 300000000; // 300MB\n        this.yieldThreadTime = yieldThreadTime;\n    }\n    async getHeader(origOpts = {}) {\n        const opts = makeOpts(origOpts);\n        const indexData = await this.index.parse(opts);\n        const ret = indexData.firstDataLine\n            ? indexData.firstDataLine.blockPosition + 65535\n            : undefined;\n        let buffer;\n        if (ret) {\n            const res = await this.bam.read(Buffer.alloc(ret + blockLen), 0, ret + blockLen, 0, opts);\n            const { bytesRead } = res;\n            ({ buffer } = res);\n            if (!bytesRead) {\n                throw new Error('Error reading header');\n            }\n            if (bytesRead < ret) {\n                buffer = buffer.subarray(0, bytesRead);\n            }\n            else {\n                buffer = buffer.subarray(0, ret);\n            }\n        }\n        else {\n            buffer = (await this.bam.readFile(opts));\n        }\n        const uncba = await unzip(buffer);\n        if (uncba.readInt32LE(0) !== BAM_MAGIC) {\n            throw new Error('Not a BAM file');\n        }\n        const headLen = uncba.readInt32LE(4);\n        this.header = uncba.toString('utf8', 8, 8 + headLen);\n        const { chrToIndex, indexToChr } = await this._readRefSeqs(headLen + 8, 65535, opts);\n        this.chrToIndex = chrToIndex;\n        this.indexToChr = indexToChr;\n        return parseHeaderText(this.header);\n    }\n    async getHeaderText(opts = {}) {\n        await this.getHeader(opts);\n        return this.header;\n    }\n    // the full length of the refseq block is not given in advance so this grabs\n    // a chunk and doubles it if all refseqs haven't been processed\n    async _readRefSeqs(start, refSeqBytes, opts = {}) {\n        if (start > refSeqBytes) {\n            return this._readRefSeqs(start, refSeqBytes * 2, opts);\n        }\n        const size = refSeqBytes + blockLen;\n        const { bytesRead, buffer } = await this.bam.read(Buffer.alloc(size), 0, refSeqBytes, 0, opts);\n        if (!bytesRead) {\n            throw new Error('Error reading refseqs from header');\n        }\n        const uncba = await unzip(buffer.subarray(0, Math.min(bytesRead, refSeqBytes)));\n        const nRef = uncba.readInt32LE(start);\n        let p = start + 4;\n        const chrToIndex = {};\n        const indexToChr = [];\n        for (let i = 0; i < nRef; i += 1) {\n            const lName = uncba.readInt32LE(p);\n            const refName = this.renameRefSeq(uncba.toString('utf8', p + 4, p + 4 + lName - 1));\n            const lRef = uncba.readInt32LE(p + lName + 4);\n            chrToIndex[refName] = i;\n            indexToChr.push({ refName, length: lRef });\n            p = p + 8 + lName;\n            if (p > uncba.length) {\n                console.warn(`BAM header is very big.  Re-fetching ${refSeqBytes} bytes.`);\n                return this._readRefSeqs(start, refSeqBytes * 2, opts);\n            }\n        }\n        return { chrToIndex, indexToChr };\n    }\n    async getRecordsForRange(chr, min, max, opts = {\n        viewAsPairs: false,\n        pairAcrossChr: false,\n        maxInsertSize: 200000,\n    }) {\n        return flat(await gen2array(this.streamRecordsForRange(chr, min, max, opts)));\n    }\n    async *streamRecordsForRange(chr, min, max, opts = {}) {\n        const { signal } = opts;\n        const chrId = this.chrToIndex && this.chrToIndex[chr];\n        let chunks;\n        if (!(chrId >= 0)) {\n            chunks = [];\n        }\n        else {\n            chunks = await this.index.blocksForRange(chrId, min - 1, max, opts);\n            if (!chunks) {\n                throw new Error('Error in index fetch');\n            }\n        }\n        for (let i = 0; i < chunks.length; i += 1) {\n            await abortBreakPoint(signal);\n            const size = chunks[i].fetchedSize();\n            if (size > this.chunkSizeLimit) {\n                throw new Error(`Too many BAM features. BAM chunk size ${size} bytes exceeds chunkSizeLimit of ${this.chunkSizeLimit}`);\n            }\n        }\n        const totalSize = chunks\n            .map(s => s.fetchedSize())\n            .reduce((a, b) => a + b, 0);\n        if (totalSize > this.fetchSizeLimit) {\n            throw new Error(`data size of ${totalSize.toLocaleString()} bytes exceeded fetch size limit of ${this.fetchSizeLimit.toLocaleString()} bytes`);\n        }\n        yield* this._fetchChunkFeatures(chunks, chrId, min, max, opts);\n    }\n    async *_fetchChunkFeatures(chunks, chrId, min, max, opts) {\n        const { viewAsPairs = false } = opts;\n        const feats = [];\n        let done = false;\n        for (let i = 0; i < chunks.length; i++) {\n            const c = chunks[i];\n            const records = (await this.featureCache.get(c.toString(), {\n                chunk: c,\n                opts,\n            }, opts.signal));\n            const recs = [];\n            for (let i = 0; i < records.length; i += 1) {\n                const feature = records[i];\n                if (feature.seq_id() === chrId) {\n                    if (feature.get('start') >= max) {\n                        // past end of range, can stop iterating\n                        done = true;\n                        break;\n                    }\n                    else if (feature.get('end') >= min) {\n                        // must be in range\n                        recs.push(feature);\n                    }\n                }\n            }\n            feats.push(recs);\n            yield recs;\n            if (done) {\n                break;\n            }\n        }\n        checkAbortSignal(opts.signal);\n        if (viewAsPairs) {\n            yield this.fetchPairs(chrId, feats, opts);\n        }\n    }\n    async fetchPairs(chrId, feats, opts) {\n        const { pairAcrossChr = false, maxInsertSize = 200000 } = opts;\n        const unmatedPairs = {};\n        const readIds = {};\n        feats.map(ret => {\n            const readNames = {};\n            for (let i = 0; i < ret.length; i++) {\n                const name = ret[i].name();\n                const id = ret[i].id();\n                if (!readNames[name]) {\n                    readNames[name] = 0;\n                }\n                readNames[name]++;\n                readIds[id] = 1;\n            }\n            entries(readNames).forEach(([k, v]) => {\n                if (v === 1) {\n                    unmatedPairs[k] = true;\n                }\n            });\n        });\n        const matePromises = [];\n        feats.map(ret => {\n            for (let i = 0; i < ret.length; i++) {\n                const f = ret[i];\n                const name = f.name();\n                const start = f.get('start');\n                const pnext = f._next_pos();\n                const rnext = f._next_refid();\n                if (unmatedPairs[name] &&\n                    (pairAcrossChr ||\n                        (rnext === chrId && Math.abs(start - pnext) < maxInsertSize))) {\n                    matePromises.push(this.index.blocksForRange(rnext, pnext, pnext + 1, opts));\n                }\n            }\n        });\n        // filter out duplicate chunks (the blocks are lists of chunks, blocks are\n        // concatenated, then filter dup chunks)\n        const mateChunks = flat(await Promise.all(matePromises))\n            .sort()\n            .filter((item, pos, ary) => !pos || item.toString() !== ary[pos - 1].toString());\n        const mateTotalSize = mateChunks\n            .map(s => s.fetchedSize())\n            .reduce((a, b) => a + b, 0);\n        if (mateTotalSize > this.fetchSizeLimit) {\n            throw new Error(`data size of ${mateTotalSize.toLocaleString()} bytes exceeded fetch size limit of ${this.fetchSizeLimit.toLocaleString()} bytes`);\n        }\n        const mateFeatPromises = mateChunks.map(async (c) => {\n            const { data, cpositions, dpositions, chunk } = await this._readChunk({\n                chunk: c,\n                opts,\n            });\n            const feats = await this.readBamFeatures(data, cpositions, dpositions, chunk);\n            const mateRecs = [];\n            for (let i = 0; i < feats.length; i += 1) {\n                const feature = feats[i];\n                if (unmatedPairs[feature.get('name')] && !readIds[feature.id()]) {\n                    mateRecs.push(feature);\n                }\n            }\n            return mateRecs;\n        });\n        return flat(await Promise.all(mateFeatPromises));\n    }\n    async _readChunk({ chunk, opts }) {\n        const size = chunk.fetchedSize();\n        const { buffer, bytesRead } = await this.bam.read(Buffer.alloc(size), 0, size, chunk.minv.blockPosition, opts);\n        const { buffer: data, cpositions, dpositions, } = await unzipChunkSlice(buffer.subarray(0, Math.min(bytesRead, size)), chunk);\n        return { data, cpositions, dpositions, chunk };\n    }\n    async readBamFeatures(ba, cpositions, dpositions, chunk) {\n        let blockStart = 0;\n        const sink = [];\n        let pos = 0;\n        let last = +Date.now();\n        while (blockStart + 4 < ba.length) {\n            const blockSize = ba.readInt32LE(blockStart);\n            const blockEnd = blockStart + 4 + blockSize - 1;\n            // increment position to the current decompressed status\n            if (dpositions) {\n                while (blockStart + chunk.minv.dataPosition >= dpositions[pos++]) { }\n                pos--;\n            }\n            // only try to read the feature if we have all the bytes for it\n            if (blockEnd < ba.length) {\n                const feature = new BAMFeature({\n                    bytes: {\n                        byteArray: ba,\n                        start: blockStart,\n                        end: blockEnd,\n                    },\n                    // the below results in an automatically calculated file-offset based ID\n                    // if the info for that is available, otherwise crc32 of the features\n                    //\n                    // cpositions[pos] refers to actual file offset of a bgzip block boundaries\n                    //\n                    // we multiply by (1 <<8) in order to make sure each block has a \"unique\"\n                    // address space so that data in that block could never overlap\n                    //\n                    // then the blockStart-dpositions is an uncompressed file offset from\n                    // that bgzip block boundary, and since the cpositions are multiplied by\n                    // (1 << 8) these uncompressed offsets get a unique space\n                    //\n                    // this has an extra chunk.minv.dataPosition added on because it blockStart\n                    // starts at 0 instead of chunk.minv.dataPosition\n                    //\n                    // the +1 is just to avoid any possible uniqueId 0 but this does not realistically happen\n                    fileOffset: cpositions\n                        ? cpositions[pos] * (1 << 8) +\n                            (blockStart - dpositions[pos]) +\n                            chunk.minv.dataPosition +\n                            1\n                        : // must be slice, not subarray for buffer polyfill on web\n                            crc32.signed(ba.slice(blockStart, blockEnd)),\n                });\n                sink.push(feature);\n                if (this.yieldThreadTime && +Date.now() - last > this.yieldThreadTime) {\n                    await timeout(1);\n                    last = +Date.now();\n                }\n            }\n            blockStart = blockEnd + 1;\n        }\n        return sink;\n    }\n    async hasRefSeq(seqName) {\n        const refId = this.chrToIndex && this.chrToIndex[seqName];\n        return this.index.hasRefSeq(refId);\n    }\n    async lineCount(seqName) {\n        const refId = this.chrToIndex && this.chrToIndex[seqName];\n        return this.index.lineCount(refId);\n    }\n    async indexCov(seqName, start, end) {\n        await this.index.parse();\n        const seqId = this.chrToIndex && this.chrToIndex[seqName];\n        return this.index.indexCov(seqId, start, end);\n    }\n    async blocksForRange(seqName, start, end, opts) {\n        await this.index.parse();\n        const seqId = this.chrToIndex && this.chrToIndex[seqName];\n        return this.index.blocksForRange(seqId, start, end, opts);\n    }\n}\n//# sourceMappingURL=bamFile.js.map","// little class representing a chunk in the index\nexport default class Chunk {\n    /**\n     * @param {VirtualOffset} minv\n     * @param {VirtualOffset} maxv\n     * @param {number} bin\n     * @param {number} [fetchedSize]\n     */\n    constructor(minv, maxv, bin, fetchedSize = undefined) {\n        this.minv = minv;\n        this.maxv = maxv;\n        this.bin = bin;\n        this._fetchedSize = fetchedSize;\n    }\n    toUniqueString() {\n        return `${this.minv}..${this.maxv} (bin ${this.bin}, fetchedSize ${this.fetchedSize()})`;\n    }\n    toString() {\n        return this.toUniqueString();\n    }\n    compareTo(b) {\n        return (this.minv.compareTo(b.minv) ||\n            this.maxv.compareTo(b.maxv) ||\n            this.bin - b.bin);\n    }\n    fetchedSize() {\n        if (this._fetchedSize !== undefined) {\n            return this._fetchedSize;\n        }\n        return this.maxv.blockPosition + (1 << 16) - this.minv.blockPosition;\n    }\n}\n//# sourceMappingURL=chunk.js.map","export default {\n    //  the read is paired in sequencing, no matter whether it is mapped in a pair\n    BAM_FPAIRED: 1,\n    //  the read is mapped in a proper pair\n    BAM_FPROPER_PAIR: 2,\n    //  the read itself is unmapped; conflictive with BAM_FPROPER_PAIR\n    BAM_FUNMAP: 4,\n    //  the mate is unmapped\n    BAM_FMUNMAP: 8,\n    //  the read is mapped to the reverse strand\n    BAM_FREVERSE: 16,\n    //  the mate is mapped to the reverse strand\n    BAM_FMREVERSE: 32,\n    //  this is read1\n    BAM_FREAD1: 64,\n    //  this is read2\n    BAM_FREAD2: 128,\n    //  not primary alignment\n    BAM_FSECONDARY: 256,\n    //  QC failure\n    BAM_FQCFAIL: 512,\n    //  optical or PCR duplicate\n    BAM_FDUP: 1024,\n    //  supplementary alignment\n    BAM_FSUPPLEMENTARY: 2048,\n};\n//# sourceMappingURL=constants.js.map","import Long from 'long';\nimport { unzip } from '@gmod/bgzf-filehandle';\nimport VirtualOffset, { fromBytes } from './virtualOffset';\nimport Chunk from './chunk';\nimport { longToNumber, abortBreakPoint, optimizeChunks } from './util';\nimport IndexFile from './indexFile';\nconst CSI1_MAGIC = 21582659; // CSI\\1\nconst CSI2_MAGIC = 38359875; // CSI\\2\nfunction lshift(num, bits) {\n    return num * 2 ** bits;\n}\nfunction rshift(num, bits) {\n    return Math.floor(num / 2 ** bits);\n}\nexport default class CSI extends IndexFile {\n    constructor(args) {\n        super(args);\n        this.maxBinNumber = 0;\n        this.depth = 0;\n        this.minShift = 0;\n    }\n    async lineCount(refId) {\n        const indexData = await this.parse();\n        if (!indexData) {\n            return -1;\n        }\n        const idx = indexData.indices[refId];\n        if (!idx) {\n            return -1;\n        }\n        const { stats } = indexData.indices[refId];\n        if (stats) {\n            return stats.lineCount;\n        }\n        return -1;\n    }\n    async indexCov() {\n        return [];\n    }\n    parseAuxData(bytes, offset, auxLength) {\n        if (auxLength < 30) {\n            return {};\n        }\n        const data = {};\n        data.formatFlags = bytes.readInt32LE(offset);\n        data.coordinateType =\n            data.formatFlags & 0x10000 ? 'zero-based-half-open' : '1-based-closed';\n        data.format = { 0: 'generic', 1: 'SAM', 2: 'VCF' }[data.formatFlags & 0xf];\n        if (!data.format) {\n            throw new Error(`invalid Tabix preset format flags ${data.formatFlags}`);\n        }\n        data.columnNumbers = {\n            ref: bytes.readInt32LE(offset + 4),\n            start: bytes.readInt32LE(offset + 8),\n            end: bytes.readInt32LE(offset + 12),\n        };\n        data.metaValue = bytes.readInt32LE(offset + 16);\n        data.metaChar = data.metaValue ? String.fromCharCode(data.metaValue) : '';\n        data.skipLines = bytes.readInt32LE(offset + 20);\n        const nameSectionLength = bytes.readInt32LE(offset + 24);\n        Object.assign(data, this._parseNameBytes(bytes.subarray(offset + 28, offset + 28 + nameSectionLength)));\n        return data;\n    }\n    _parseNameBytes(namesBytes) {\n        let currRefId = 0;\n        let currNameStart = 0;\n        const refIdToName = [];\n        const refNameToId = {};\n        for (let i = 0; i < namesBytes.length; i += 1) {\n            if (!namesBytes[i]) {\n                if (currNameStart < i) {\n                    let refName = namesBytes.toString('utf8', currNameStart, i);\n                    refName = this.renameRefSeq(refName);\n                    refIdToName[currRefId] = refName;\n                    refNameToId[refName] = currRefId;\n                }\n                currNameStart = i + 1;\n                currRefId += 1;\n            }\n        }\n        return { refNameToId, refIdToName };\n    }\n    // fetch and parse the index\n    async _parse(opts) {\n        const data = { csi: true, maxBlockSize: 1 << 16 };\n        const buffer = (await this.filehandle.readFile(opts));\n        const bytes = await unzip(buffer);\n        // check TBI magic numbers\n        if (bytes.readUInt32LE(0) === CSI1_MAGIC) {\n            data.csiVersion = 1;\n        }\n        else if (bytes.readUInt32LE(0) === CSI2_MAGIC) {\n            data.csiVersion = 2;\n        }\n        else {\n            throw new Error('Not a CSI file');\n            // TODO: do we need to support big-endian CSI files?\n        }\n        this.minShift = bytes.readInt32LE(4);\n        this.depth = bytes.readInt32LE(8);\n        this.maxBinNumber = ((1 << ((this.depth + 1) * 3)) - 1) / 7;\n        const auxLength = bytes.readInt32LE(12);\n        if (auxLength) {\n            Object.assign(data, this.parseAuxData(bytes, 16, auxLength));\n        }\n        data.refCount = bytes.readInt32LE(16 + auxLength);\n        // read the indexes for each reference sequence\n        data.indices = new Array(data.refCount);\n        let currOffset = 16 + auxLength + 4;\n        for (let i = 0; i < data.refCount; i += 1) {\n            await abortBreakPoint(opts.signal);\n            // the binning index\n            const binCount = bytes.readInt32LE(currOffset);\n            currOffset += 4;\n            const binIndex = {};\n            let stats; // < provided by parsing a pseudo-bin, if present\n            for (let j = 0; j < binCount; j += 1) {\n                const bin = bytes.readUInt32LE(currOffset);\n                if (bin > this.maxBinNumber) {\n                    // this is a fake bin that actually has stats information\n                    // about the reference sequence in it\n                    stats = this.parsePseudoBin(bytes, currOffset + 4);\n                    currOffset += 4 + 8 + 4 + 16 + 16;\n                }\n                else {\n                    const loffset = fromBytes(bytes, currOffset + 4);\n                    this._findFirstData(data, loffset);\n                    const chunkCount = bytes.readInt32LE(currOffset + 12);\n                    currOffset += 16;\n                    const chunks = new Array(chunkCount);\n                    for (let k = 0; k < chunkCount; k += 1) {\n                        const u = fromBytes(bytes, currOffset);\n                        const v = fromBytes(bytes, currOffset + 8);\n                        currOffset += 16;\n                        // this._findFirstData(data, u)\n                        chunks[k] = new Chunk(u, v, bin);\n                    }\n                    binIndex[bin] = chunks;\n                }\n            }\n            data.indices[i] = { binIndex, stats };\n        }\n        return data;\n    }\n    parsePseudoBin(bytes, offset) {\n        const lineCount = longToNumber(Long.fromBytesLE(Array.prototype.slice.call(bytes, offset + 28, offset + 36), true));\n        return { lineCount };\n    }\n    async blocksForRange(refId, min, max, opts = {}) {\n        if (min < 0) {\n            min = 0;\n        }\n        const indexData = await this.parse(opts);\n        if (!indexData) {\n            return [];\n        }\n        const ba = indexData.indices[refId];\n        if (!ba) {\n            return [];\n        }\n        const overlappingBins = this.reg2bins(min, max); // List of bin #s that overlap min, max\n        const chunks = [];\n        // Find chunks in overlapping bins.  Leaf bins (< 4681) are not pruned\n        for (const [start, end] of overlappingBins) {\n            for (let bin = start; bin <= end; bin++) {\n                if (ba.binIndex[bin]) {\n                    const binChunks = ba.binIndex[bin];\n                    for (let c = 0; c < binChunks.length; ++c) {\n                        chunks.push(new Chunk(binChunks[c].minv, binChunks[c].maxv, bin));\n                    }\n                }\n            }\n        }\n        return optimizeChunks(chunks, new VirtualOffset(0, 0));\n    }\n    /**\n     * calculate the list of bins that may overlap with region [beg,end) (zero-based half-open)\n     * @returns {Array[number]}\n     */\n    reg2bins(beg, end) {\n        beg -= 1; // < convert to 1-based closed\n        if (beg < 1) {\n            beg = 1;\n        }\n        if (end > 2 ** 50) {\n            end = 2 ** 34;\n        } // 17 GiB ought to be enough for anybody\n        end -= 1;\n        let l = 0;\n        let t = 0;\n        let s = this.minShift + this.depth * 3;\n        const bins = [];\n        for (; l <= this.depth; s -= 3, t += lshift(1, l * 3), l += 1) {\n            const b = t + rshift(beg, s);\n            const e = t + rshift(end, s);\n            if (e - b + bins.length > this.maxBinNumber) {\n                throw new Error(`query ${beg}-${end} is too large for current binning scheme (shift ${this.minShift}, depth ${this.depth}), try a smaller query or a coarser index binning scheme`);\n            }\n            bins.push([b, e]);\n        }\n        return bins;\n    }\n}\n//# sourceMappingURL=csi.js.map","import BamFile, { BAM_MAGIC } from './bamFile';\nimport 'cross-fetch/polyfill';\nimport { unzip } from '@gmod/bgzf-filehandle';\nimport { parseHeaderText } from './sam';\nasync function concat(arr, opts) {\n    const res = await Promise.all(arr.map(async (chunk) => {\n        const { url, headers } = chunk;\n        if (url.startsWith('data:')) {\n            return Buffer.from(url.split(',')[1], 'base64');\n        }\n        else {\n            //remove referer header, it is not even allowed to be specified\n            //@ts-ignore\n            //eslint-disable-next-line @typescript-eslint/no-unused-vars\n            const { referer, ...rest } = headers;\n            const res = await fetch(url, {\n                ...opts,\n                headers: { ...opts.headers, ...rest },\n            });\n            if (!res.ok) {\n                throw new Error(`Failed to fetch ${res.statusText}`);\n            }\n            return Buffer.from(await res.arrayBuffer());\n        }\n    }));\n    return Buffer.concat(await Promise.all(res.map(elt => unzip(elt))));\n}\nexport default class HtsgetFile extends BamFile {\n    constructor(args) {\n        // @ts-ignore override bam defaults\n        super({ bamFilehandle: '?', baiFilehandle: '?' });\n        this.baseUrl = args.baseUrl;\n        this.trackId = args.trackId;\n    }\n    async *streamRecordsForRange(chr, min, max, opts = {\n        viewAsPairs: false,\n        pairAcrossChr: false,\n        maxInsertSize: 200000,\n    }) {\n        const base = `${this.baseUrl}/${this.trackId}`;\n        const url = `${base}?referenceName=${chr}&start=${min}&end=${max}&format=BAM`;\n        const chrId = this.chrToIndex && this.chrToIndex[chr];\n        const result = await fetch(url, { ...opts });\n        if (!result.ok) {\n            throw new Error(result.statusText);\n        }\n        const data = await result.json();\n        const uncba = await concat(data.htsget.urls.slice(1), opts);\n        const chunk = {\n            buffer: uncba,\n            chunk: { minv: { dataPosition: 0 } },\n            toString() {\n                return `${chr}_${min}_${max}`;\n            },\n        };\n        yield* this._fetchChunkFeatures(\n        // @ts-ignore\n        [chunk], chrId, min, max, opts);\n    }\n    //@ts-ignore\n    async _readChunk(params) {\n        const { chunk } = params;\n        const { buffer, chunk: c2 } = chunk;\n        return { data: buffer, cpositions: null, dpositions: null, chunk: c2 };\n    }\n    async getHeader(opts = {}) {\n        const url = `${this.baseUrl}/${this.trackId}?referenceName=na&class=header`;\n        const result = await fetch(url, opts);\n        if (!result.ok) {\n            throw new Error(`Failed to fetch ${result.statusText}`);\n        }\n        const data = await result.json();\n        const uncba = await concat(data.htsget.urls, opts);\n        if (uncba.readInt32LE(0) !== BAM_MAGIC) {\n            throw new Error('Not a BAM file');\n        }\n        const headLen = uncba.readInt32LE(4);\n        const headerText = uncba.toString('utf8', 8, 8 + headLen);\n        const samHeader = parseHeaderText(headerText);\n        // use the @SQ lines in the header to figure out the\n        // mapping between ref ref ID numbers and names\n        const idToName = [];\n        const nameToId = {};\n        const sqLines = samHeader.filter((l) => l.tag === 'SQ');\n        sqLines.forEach((sqLine, refId) => {\n            sqLine.data.forEach((item) => {\n                if (item.tag === 'SN') {\n                    // this is the ref name\n                    const refName = item.value;\n                    nameToId[refName] = refId;\n                    idToName[refId] = refName;\n                }\n            });\n        });\n        this.chrToIndex = nameToId;\n        this.indexToChr = idToName;\n        return samHeader;\n    }\n}\n//# sourceMappingURL=htsget.js.map","import BAI from './bai';\nimport CSI from './csi';\nimport BamFile from './bamFile';\nimport HtsgetFile from './htsget';\nimport BamRecord from './record';\nexport { BAI, CSI, BamFile, BamRecord, HtsgetFile };\n//# sourceMappingURL=index.js.map","export default class IndexFile {\n    /**\n     * @param {filehandle} filehandle\n     * @param {function} [renameRefSeqs]\n     */\n    constructor({ filehandle, renameRefSeq = (n) => n, }) {\n        this.filehandle = filehandle;\n        this.renameRefSeq = renameRefSeq;\n    }\n    _findFirstData(data, virtualOffset) {\n        const currentFdl = data.firstDataLine;\n        if (currentFdl) {\n            data.firstDataLine =\n                currentFdl.compareTo(virtualOffset) > 0 ? virtualOffset : currentFdl;\n        }\n        else {\n            data.firstDataLine = virtualOffset;\n        }\n    }\n    async parse(opts = {}) {\n        if (!this.setupP) {\n            this.setupP = this._parse(opts).catch(e => {\n                this.setupP = undefined;\n                throw e;\n            });\n        }\n        return this.setupP;\n    }\n    async hasRefSeq(seqId, opts = {}) {\n        return !!((await this.parse(opts)).indices[seqId] || {}).binIndex;\n    }\n}\n//# sourceMappingURL=indexFile.js.map","/* eslint-disable @typescript-eslint/no-empty-function */\nimport Constants from './constants';\nconst SEQRET_DECODER = '=ACMGRSVTWYHKDBN'.split('');\nconst CIGAR_DECODER = 'MIDNSHP=X???????'.split('');\n/**\n * Class of each BAM record returned by this API.\n */\nexport default class BamRecord {\n    constructor(args) {\n        this.data = {};\n        this._tagList = [];\n        this._allTagsParsed = false;\n        const { bytes, fileOffset } = args;\n        const { byteArray, start } = bytes;\n        this.data = {};\n        this.bytes = bytes;\n        this._id = fileOffset;\n        this._refID = byteArray.readInt32LE(start + 4);\n        this.data.start = byteArray.readInt32LE(start + 8);\n        this.flags = (byteArray.readInt32LE(start + 16) & 0xffff0000) >> 16;\n    }\n    get(field) {\n        //@ts-ignore\n        if (this[field]) {\n            //@ts-ignore\n            if (this.data[field]) {\n                return this.data[field];\n            }\n            //@ts-ignore\n            this.data[field] = this[field]();\n            return this.data[field];\n        }\n        return this._get(field.toLowerCase());\n    }\n    end() {\n        return this.get('start') + this.get('length_on_ref');\n    }\n    seq_id() {\n        return this._refID;\n    }\n    // same as get(), except requires lower-case arguments.  used\n    // internally to save lots of calls to field.toLowerCase()\n    _get(field) {\n        if (field in this.data) {\n            return this.data[field];\n        }\n        this.data[field] = this._parseTag(field);\n        return this.data[field];\n    }\n    _tags() {\n        this._parseAllTags();\n        let tags = ['seq'];\n        if (!this.isSegmentUnmapped()) {\n            tags.push('start', 'end', 'strand', 'score', 'qual', 'MQ', 'CIGAR', 'length_on_ref', 'template_length');\n        }\n        if (this.isPaired()) {\n            tags.push('next_segment_position', 'pair_orientation');\n        }\n        tags = tags.concat(this._tagList || []);\n        Object.keys(this.data).forEach(k => {\n            if (k[0] !== '_' && k !== 'next_seq_id') {\n                tags.push(k);\n            }\n        });\n        const seen = {};\n        return tags.filter(t => {\n            if ((t in this.data && this.data[t] === undefined) ||\n                t === 'CG' ||\n                t === 'cg') {\n                return false;\n            }\n            const lt = t.toLowerCase();\n            const s = seen[lt];\n            seen[lt] = true;\n            return !s;\n        });\n    }\n    parent() {\n        return undefined;\n    }\n    children() {\n        return this.get('subfeatures');\n    }\n    id() {\n        return this._id;\n    }\n    // special parsers\n    /**\n     * Mapping quality score.\n     */\n    mq() {\n        const mq = (this.get('_bin_mq_nl') & 0xff00) >> 8;\n        return mq === 255 ? undefined : mq;\n    }\n    score() {\n        return this.get('mq');\n    }\n    qual() {\n        var _a;\n        return (_a = this.qualRaw()) === null || _a === void 0 ? void 0 : _a.join(' ');\n    }\n    qualRaw() {\n        if (this.isSegmentUnmapped()) {\n            return undefined;\n        }\n        const { start, byteArray } = this.bytes;\n        const p = start +\n            36 +\n            this.get('_l_read_name') +\n            this.get('_n_cigar_op') * 4 +\n            this.get('_seq_bytes');\n        const lseq = this.get('seq_length');\n        return byteArray.subarray(p, p + lseq);\n    }\n    strand() {\n        return this.isReverseComplemented() ? -1 : 1;\n    }\n    multi_segment_next_segment_strand() {\n        if (this.isMateUnmapped()) {\n            return undefined;\n        }\n        return this.isMateReverseComplemented() ? -1 : 1;\n    }\n    name() {\n        return this.get('_read_name');\n    }\n    _read_name() {\n        const nl = this.get('_l_read_name');\n        const { byteArray, start } = this.bytes;\n        return byteArray.toString('ascii', start + 36, start + 36 + nl - 1);\n    }\n    /**\n     * Get the value of a tag, parsing the tags as far as necessary.\n     * Only called if we have not already parsed that field.\n     */\n    _parseTag(tagName) {\n        // if all of the tags have been parsed and we're still being\n        // called, we already know that we have no such tag, because\n        // it would already have been cached.\n        if (this._allTagsParsed) {\n            return undefined;\n        }\n        const { byteArray, start } = this.bytes;\n        let p = this._tagOffset ||\n            start +\n                36 +\n                this.get('_l_read_name') +\n                this.get('_n_cigar_op') * 4 +\n                this.get('_seq_bytes') +\n                this.get('seq_length');\n        const blockEnd = this.bytes.end;\n        let lcTag;\n        while (p < blockEnd && lcTag !== tagName) {\n            const tag = String.fromCharCode(byteArray[p], byteArray[p + 1]);\n            lcTag = tag.toLowerCase();\n            const type = String.fromCharCode(byteArray[p + 2]);\n            p += 3;\n            let value;\n            switch (type) {\n                case 'A':\n                    value = String.fromCharCode(byteArray[p]);\n                    p += 1;\n                    break;\n                case 'i':\n                    value = byteArray.readInt32LE(p);\n                    p += 4;\n                    break;\n                case 'I':\n                    value = byteArray.readUInt32LE(p);\n                    p += 4;\n                    break;\n                case 'c':\n                    value = byteArray.readInt8(p);\n                    p += 1;\n                    break;\n                case 'C':\n                    value = byteArray.readUInt8(p);\n                    p += 1;\n                    break;\n                case 's':\n                    value = byteArray.readInt16LE(p);\n                    p += 2;\n                    break;\n                case 'S':\n                    value = byteArray.readUInt16LE(p);\n                    p += 2;\n                    break;\n                case 'f':\n                    value = byteArray.readFloatLE(p);\n                    p += 4;\n                    break;\n                case 'Z':\n                case 'H':\n                    value = '';\n                    while (p <= blockEnd) {\n                        const cc = byteArray[p++];\n                        if (cc === 0) {\n                            break;\n                        }\n                        else {\n                            value += String.fromCharCode(cc);\n                        }\n                    }\n                    break;\n                case 'B': {\n                    value = '';\n                    const cc = byteArray[p++];\n                    const Btype = String.fromCharCode(cc);\n                    const limit = byteArray.readInt32LE(p);\n                    p += 4;\n                    if (Btype === 'i') {\n                        if (tag === 'CG') {\n                            for (let k = 0; k < limit; k++) {\n                                const cigop = byteArray.readInt32LE(p);\n                                const lop = cigop >> 4;\n                                const op = CIGAR_DECODER[cigop & 0xf];\n                                value += lop + op;\n                                p += 4;\n                            }\n                        }\n                        else {\n                            for (let k = 0; k < limit; k++) {\n                                value += byteArray.readInt32LE(p);\n                                if (k + 1 < limit) {\n                                    value += ',';\n                                }\n                                p += 4;\n                            }\n                        }\n                    }\n                    if (Btype === 'I') {\n                        if (tag === 'CG') {\n                            for (let k = 0; k < limit; k++) {\n                                const cigop = byteArray.readUInt32LE(p);\n                                const lop = cigop >> 4;\n                                const op = CIGAR_DECODER[cigop & 0xf];\n                                value += lop + op;\n                                p += 4;\n                            }\n                        }\n                        else {\n                            for (let k = 0; k < limit; k++) {\n                                value += byteArray.readUInt32LE(p);\n                                if (k + 1 < limit) {\n                                    value += ',';\n                                }\n                                p += 4;\n                            }\n                        }\n                    }\n                    if (Btype === 's') {\n                        for (let k = 0; k < limit; k++) {\n                            value += byteArray.readInt16LE(p);\n                            if (k + 1 < limit) {\n                                value += ',';\n                            }\n                            p += 2;\n                        }\n                    }\n                    if (Btype === 'S') {\n                        for (let k = 0; k < limit; k++) {\n                            value += byteArray.readUInt16LE(p);\n                            if (k + 1 < limit) {\n                                value += ',';\n                            }\n                            p += 2;\n                        }\n                    }\n                    if (Btype === 'c') {\n                        for (let k = 0; k < limit; k++) {\n                            value += byteArray.readInt8(p);\n                            if (k + 1 < limit) {\n                                value += ',';\n                            }\n                            p += 1;\n                        }\n                    }\n                    if (Btype === 'C') {\n                        for (let k = 0; k < limit; k++) {\n                            value += byteArray.readUInt8(p);\n                            if (k + 1 < limit) {\n                                value += ',';\n                            }\n                            p += 1;\n                        }\n                    }\n                    if (Btype === 'f') {\n                        for (let k = 0; k < limit; k++) {\n                            value += byteArray.readFloatLE(p);\n                            if (k + 1 < limit) {\n                                value += ',';\n                            }\n                            p += 4;\n                        }\n                    }\n                    break;\n                }\n                default:\n                    console.warn(`Unknown BAM tag type '${type}', tags may be incomplete`);\n                    value = undefined;\n                    p = blockEnd; // stop parsing tags\n            }\n            this._tagOffset = p;\n            this._tagList.push(tag);\n            if (lcTag === tagName) {\n                return value;\n            }\n            this.data[lcTag] = value;\n        }\n        this._allTagsParsed = true;\n        return undefined;\n    }\n    _parseAllTags() {\n        this._parseTag('');\n    }\n    _parseCigar(cigar) {\n        return (\n        //@ts-ignore\n        cigar\n            .match(/\\d+\\D/g)\n            //@ts-ignore\n            .map(op => [op.match(/\\D/)[0].toUpperCase(), parseInt(op, 10)]));\n    }\n    /**\n     * @returns {boolean} true if the read is paired, regardless of whether both segments are mapped\n     */\n    isPaired() {\n        return !!(this.flags & Constants.BAM_FPAIRED);\n    }\n    /** @returns {boolean} true if the read is paired, and both segments are mapped */\n    isProperlyPaired() {\n        return !!(this.flags & Constants.BAM_FPROPER_PAIR);\n    }\n    /** @returns {boolean} true if the read itself is unmapped; conflictive with isProperlyPaired */\n    isSegmentUnmapped() {\n        return !!(this.flags & Constants.BAM_FUNMAP);\n    }\n    /** @returns {boolean} true if the read itself is unmapped; conflictive with isProperlyPaired */\n    isMateUnmapped() {\n        return !!(this.flags & Constants.BAM_FMUNMAP);\n    }\n    /** @returns {boolean} true if the read is mapped to the reverse strand */\n    isReverseComplemented() {\n        return !!(this.flags & Constants.BAM_FREVERSE);\n    }\n    /** @returns {boolean} true if the mate is mapped to the reverse strand */\n    isMateReverseComplemented() {\n        return !!(this.flags & Constants.BAM_FMREVERSE);\n    }\n    /** @returns {boolean} true if this is read number 1 in a pair */\n    isRead1() {\n        return !!(this.flags & Constants.BAM_FREAD1);\n    }\n    /** @returns {boolean} true if this is read number 2 in a pair */\n    isRead2() {\n        return !!(this.flags & Constants.BAM_FREAD2);\n    }\n    /** @returns {boolean} true if this is a secondary alignment */\n    isSecondary() {\n        return !!(this.flags & Constants.BAM_FSECONDARY);\n    }\n    /** @returns {boolean} true if this read has failed QC checks */\n    isFailedQc() {\n        return !!(this.flags & Constants.BAM_FQCFAIL);\n    }\n    /** @returns {boolean} true if the read is an optical or PCR duplicate */\n    isDuplicate() {\n        return !!(this.flags & Constants.BAM_FDUP);\n    }\n    /** @returns {boolean} true if this is a supplementary alignment */\n    isSupplementary() {\n        return !!(this.flags & Constants.BAM_FSUPPLEMENTARY);\n    }\n    cigar() {\n        if (this.isSegmentUnmapped()) {\n            return undefined;\n        }\n        const { byteArray, start } = this.bytes;\n        const numCigarOps = this.get('_n_cigar_op');\n        let p = start + 36 + this.get('_l_read_name');\n        const seqLen = this.get('seq_length');\n        let cigar = '';\n        let lref = 0;\n        // check for CG tag by inspecting whether the CIGAR field\n        // contains a clip that consumes entire seqLen\n        let cigop = byteArray.readInt32LE(p);\n        let lop = cigop >> 4;\n        let op = CIGAR_DECODER[cigop & 0xf];\n        if (op === 'S' && lop === seqLen) {\n            // if there is a CG the second CIGAR field will\n            // be a N tag the represents the length on ref\n            p += 4;\n            cigop = byteArray.readInt32LE(p);\n            lop = cigop >> 4;\n            op = CIGAR_DECODER[cigop & 0xf];\n            if (op !== 'N') {\n                console.warn('CG tag with no N tag');\n            }\n            this.data.length_on_ref = lop;\n            return this.get('CG');\n        }\n        else {\n            for (let c = 0; c < numCigarOps; ++c) {\n                cigop = byteArray.readInt32LE(p);\n                lop = cigop >> 4;\n                op = CIGAR_DECODER[cigop & 0xf];\n                cigar += lop + op;\n                // soft clip, hard clip, and insertion don't count toward\n                // the length on the reference\n                if (op !== 'H' && op !== 'S' && op !== 'I') {\n                    lref += lop;\n                }\n                p += 4;\n            }\n            this.data.length_on_ref = lref;\n            return cigar;\n        }\n    }\n    _flags() { }\n    length_on_ref() {\n        if (this.data.length_on_ref) {\n            return this.data.length_on_ref;\n        }\n        else {\n            this.get('cigar'); // the length_on_ref is set as a side effect\n            return this.data.length_on_ref;\n        }\n    }\n    _n_cigar_op() {\n        return this.get('_flag_nc') & 0xffff;\n    }\n    _l_read_name() {\n        return this.get('_bin_mq_nl') & 0xff;\n    }\n    /**\n     * number of bytes in the sequence field\n     */\n    _seq_bytes() {\n        return (this.get('seq_length') + 1) >> 1;\n    }\n    getReadBases() {\n        return this.seq();\n    }\n    seq() {\n        const { byteArray, start } = this.bytes;\n        const p = start + 36 + this.get('_l_read_name') + this.get('_n_cigar_op') * 4;\n        const seqBytes = this.get('_seq_bytes');\n        const len = this.get('seq_length');\n        let buf = '';\n        let i = 0;\n        for (let j = 0; j < seqBytes; ++j) {\n            const sb = byteArray[p + j];\n            buf += SEQRET_DECODER[(sb & 0xf0) >> 4];\n            i++;\n            if (i < len) {\n                buf += SEQRET_DECODER[sb & 0x0f];\n                i++;\n            }\n        }\n        return buf;\n    }\n    // adapted from igv.js\n    getPairOrientation() {\n        if (!this.isSegmentUnmapped() &&\n            !this.isMateUnmapped() &&\n            this._refID === this._next_refid()) {\n            const s1 = this.isReverseComplemented() ? 'R' : 'F';\n            const s2 = this.isMateReverseComplemented() ? 'R' : 'F';\n            let o1 = ' ';\n            let o2 = ' ';\n            if (this.isRead1()) {\n                o1 = '1';\n                o2 = '2';\n            }\n            else if (this.isRead2()) {\n                o1 = '2';\n                o2 = '1';\n            }\n            const tmp = [];\n            const isize = this.template_length();\n            if (isize > 0) {\n                tmp[0] = s1;\n                tmp[1] = o1;\n                tmp[2] = s2;\n                tmp[3] = o2;\n            }\n            else {\n                tmp[2] = s1;\n                tmp[3] = o1;\n                tmp[0] = s2;\n                tmp[1] = o2;\n            }\n            return tmp.join('');\n        }\n        return null;\n    }\n    _bin_mq_nl() {\n        return this.bytes.byteArray.readInt32LE(this.bytes.start + 12);\n    }\n    _flag_nc() {\n        return this.bytes.byteArray.readInt32LE(this.bytes.start + 16);\n    }\n    seq_length() {\n        return this.bytes.byteArray.readInt32LE(this.bytes.start + 20);\n    }\n    _next_refid() {\n        return this.bytes.byteArray.readInt32LE(this.bytes.start + 24);\n    }\n    _next_pos() {\n        return this.bytes.byteArray.readInt32LE(this.bytes.start + 28);\n    }\n    template_length() {\n        return this.bytes.byteArray.readInt32LE(this.bytes.start + 32);\n    }\n    toJSON() {\n        const data = {};\n        Object.keys(this).forEach(k => {\n            if (k.charAt(0) === '_' || k === 'bytes') {\n                return;\n            }\n            //@ts-ignore\n            data[k] = this[k];\n        });\n        return data;\n    }\n}\n//# sourceMappingURL=record.js.map","export function parseHeaderText(text) {\n    const lines = text.split(/\\r?\\n/);\n    const data = [];\n    lines.forEach(line => {\n        const [tag, ...fields] = line.split(/\\t/);\n        const parsedFields = fields.map(f => {\n            const [fieldTag, value] = f.split(':', 2);\n            return { tag: fieldTag, value };\n        });\n        if (tag) {\n            data.push({ tag: tag.substr(1), data: parsedFields });\n        }\n    });\n    return data;\n}\n//# sourceMappingURL=sam.js.map","export function timeout(ms) {\n    return new Promise(resolve => setTimeout(resolve, ms));\n}\nexport function longToNumber(long) {\n    if (long.greaterThan(Number.MAX_SAFE_INTEGER) ||\n        long.lessThan(Number.MIN_SAFE_INTEGER)) {\n        throw new Error('integer overflow');\n    }\n    return long.toNumber();\n}\n/**\n * Properly check if the given AbortSignal is aborted.\n * Per the standard, if the signal reads as aborted,\n * this function throws either a DOMException AbortError, or a regular error\n * with a `code` attribute set to `ERR_ABORTED`.\n *\n * For convenience, passing `undefined` is a no-op\n *\n * @param {AbortSignal} [signal] an AbortSignal, or anything with an `aborted` attribute\n * @returns nothing\n */\nexport function checkAbortSignal(signal) {\n    if (!signal) {\n        return;\n    }\n    if (signal.aborted) {\n        // console.log('bam aborted!')\n        if (typeof DOMException !== 'undefined') {\n            throw new DOMException('aborted', 'AbortError');\n        }\n        else {\n            const e = new Error('aborted');\n            //@ts-ignore\n            e.code = 'ERR_ABORTED';\n            throw e;\n        }\n    }\n}\n/**\n * Skips to the next tick, then runs `checkAbortSignal`.\n * Await this to inside an otherwise synchronous loop to\n * provide a place to break when an abort signal is received.\n * @param {AbortSignal} signal\n */\nexport async function abortBreakPoint(signal) {\n    await Promise.resolve();\n    checkAbortSignal(signal);\n}\nexport function canMergeBlocks(chunk1, chunk2) {\n    return (chunk2.minv.blockPosition - chunk1.maxv.blockPosition < 65000 &&\n        chunk2.maxv.blockPosition - chunk1.minv.blockPosition < 5000000);\n}\nexport function makeOpts(obj = {}) {\n    return 'aborted' in obj ? { signal: obj } : obj;\n}\nexport function optimizeChunks(chunks, lowest) {\n    const mergedChunks = [];\n    let lastChunk = null;\n    if (chunks.length === 0) {\n        return chunks;\n    }\n    chunks.sort((c0, c1) => {\n        const dif = c0.minv.blockPosition - c1.minv.blockPosition;\n        if (dif !== 0) {\n            return dif;\n        }\n        else {\n            return c0.minv.dataPosition - c1.minv.dataPosition;\n        }\n    });\n    chunks.forEach(chunk => {\n        if (!lowest || chunk.maxv.compareTo(lowest) > 0) {\n            if (lastChunk === null) {\n                mergedChunks.push(chunk);\n                lastChunk = chunk;\n            }\n            else {\n                if (canMergeBlocks(lastChunk, chunk)) {\n                    if (chunk.maxv.compareTo(lastChunk.maxv) > 0) {\n                        lastChunk.maxv = chunk.maxv;\n                    }\n                }\n                else {\n                    mergedChunks.push(chunk);\n                    lastChunk = chunk;\n                }\n            }\n        }\n    });\n    return mergedChunks;\n}\n//# sourceMappingURL=util.js.map","export default class VirtualOffset {\n    constructor(blockPosition, dataPosition) {\n        this.blockPosition = blockPosition; // < offset of the compressed data block\n        this.dataPosition = dataPosition; // < offset into the uncompressed data\n    }\n    toString() {\n        return `${this.blockPosition}:${this.dataPosition}`;\n    }\n    compareTo(b) {\n        return (this.blockPosition - b.blockPosition || this.dataPosition - b.dataPosition);\n    }\n    static min(...args) {\n        let min;\n        let i = 0;\n        for (; !min; i += 1) {\n            min = args[i];\n        }\n        for (; i < args.length; i += 1) {\n            if (min.compareTo(args[i]) > 0) {\n                min = args[i];\n            }\n        }\n        return min;\n    }\n}\nexport function fromBytes(bytes, offset = 0, bigendian = false) {\n    if (bigendian) {\n        throw new Error('big-endian virtual file offsets not implemented');\n    }\n    return new VirtualOffset(bytes[offset + 7] * 0x10000000000 +\n        bytes[offset + 6] * 0x100000000 +\n        bytes[offset + 5] * 0x1000000 +\n        bytes[offset + 4] * 0x10000 +\n        bytes[offset + 3] * 0x100 +\n        bytes[offset + 2], (bytes[offset + 1] << 8) | bytes[offset]);\n}\n//# sourceMappingURL=virtualOffset.js.map","import { BamFile } from '@gmod/bam';\nimport { BaseFeatureDataAdapter, } from '@jbrowse/core/data_adapters/BaseAdapter';\nimport { bytesForRegions, updateStatus } from '@jbrowse/core/util';\nimport { openLocation } from '@jbrowse/core/util/io';\nimport { ObservableCreate } from '@jbrowse/core/util/rxjs';\nimport { toArray } from 'rxjs/operators';\nimport { readConfObject } from '@jbrowse/core/configuration';\nimport BamSlightlyLazyFeature from './BamSlightlyLazyFeature';\nexport default class BamAdapter extends BaseFeatureDataAdapter {\n    // derived classes may not use the same configuration so a custom\n    // configure method allows derived classes to override this behavior\n    async configure() {\n        if (!this.configured) {\n            const bamLocation = readConfObject(this.config, 'bamLocation');\n            const location = readConfObject(this.config, ['index', 'location']);\n            const indexType = readConfObject(this.config, ['index', 'indexType']);\n            const bam = new BamFile({\n                bamFilehandle: openLocation(bamLocation, this.pluginManager),\n                csiFilehandle: indexType === 'CSI'\n                    ? openLocation(location, this.pluginManager)\n                    : undefined,\n                baiFilehandle: indexType !== 'CSI'\n                    ? openLocation(location, this.pluginManager)\n                    : undefined,\n                // chunkSizeLimit and fetchSizeLimit are more troublesome than\n                // helpful, and have given overly large values on the ultra long\n                // nanopore reads even with 500MB limits, so disabled with infinity\n                chunkSizeLimit: Infinity,\n                fetchSizeLimit: Infinity,\n                yieldThreadTime: Infinity,\n            });\n            const adapterConfig = readConfObject(this.config, 'sequenceAdapter');\n            if (adapterConfig && this.getSubAdapter) {\n                this.configured = this.getSubAdapter(adapterConfig).then(({ dataAdapter }) => ({\n                    bam,\n                    sequenceAdapter: dataAdapter,\n                }));\n            }\n            else {\n                this.configured = Promise.resolve({ bam });\n            }\n        }\n        return this.configured;\n    }\n    async getHeader(opts) {\n        const { bam } = await this.configure();\n        return bam.getHeaderText(opts);\n    }\n    async setupPre(opts) {\n        const { statusCallback = () => { } } = opts || {};\n        const { bam } = await this.configure();\n        this.samHeader = await updateStatus('Downloading index', statusCallback, async () => {\n            const samHeader = await bam.getHeader(opts);\n            // use the @SQ lines in the header to figure out the\n            // mapping between ref ref ID numbers and names\n            const idToName = [];\n            const nameToId = {};\n            samHeader\n                .filter(l => l.tag === 'SQ')\n                .forEach((sqLine, refId) => {\n                sqLine.data.forEach(item => {\n                    if (item.tag === 'SN') {\n                        // this is the ref name\n                        const refName = item.value;\n                        nameToId[refName] = refId;\n                        idToName[refId] = refName;\n                    }\n                });\n            });\n            return { idToName, nameToId };\n        });\n        return this.samHeader;\n    }\n    async setup(opts) {\n        if (!this.setupP) {\n            this.setupP = this.setupPre(opts).catch(e => {\n                this.setupP = undefined;\n                throw e;\n            });\n        }\n        return this.setupP;\n    }\n    async getRefNames(opts) {\n        const { idToName } = await this.setup(opts);\n        return idToName;\n    }\n    async seqFetch(refName, start, end) {\n        const { sequenceAdapter } = await this.configure();\n        const refSeqStore = sequenceAdapter;\n        if (!refSeqStore) {\n            return undefined;\n        }\n        if (!refName) {\n            return undefined;\n        }\n        const features = refSeqStore.getFeatures({\n            refName,\n            start,\n            end,\n            assemblyName: '',\n        });\n        const seqChunks = await features.pipe(toArray()).toPromise();\n        let sequence = '';\n        seqChunks\n            .sort((a, b) => a.get('start') - b.get('start'))\n            .forEach(chunk => {\n            const chunkStart = chunk.get('start');\n            const chunkEnd = chunk.get('end');\n            const trimStart = Math.max(start - chunkStart, 0);\n            const trimEnd = Math.min(end - chunkStart, chunkEnd - chunkStart);\n            const trimLength = trimEnd - trimStart;\n            const chunkSeq = chunk.get('seq') || chunk.get('residues');\n            sequence += chunkSeq.substr(trimStart, trimLength);\n        });\n        if (sequence.length !== end - start) {\n            throw new Error(`sequence fetch failed: fetching ${refName}:${(start - 1).toLocaleString()}-${end.toLocaleString()} returned ${sequence.length.toLocaleString()} bases, but should have returned ${(end - start).toLocaleString()}`);\n        }\n        return sequence;\n    }\n    getFeatures(region, opts) {\n        const { refName, start, end, originalRefName } = region;\n        const { signal, filterBy, statusCallback = () => { } } = opts || {};\n        return ObservableCreate(async (observer) => {\n            const { bam } = await this.configure();\n            await this.setup(opts);\n            statusCallback('Downloading alignments');\n            const records = await bam.getRecordsForRange(refName, start, end, opts);\n            const { flagInclude = 0, flagExclude = 0, tagFilter, readName, } = filterBy || {};\n            for (const record of records) {\n                let ref;\n                if (!record.get('MD')) {\n                    ref = await this.seqFetch(originalRefName || refName, record.get('start'), record.get('end'));\n                }\n                const flags = record.flags;\n                if (!((flags & flagInclude) === flagInclude && !(flags & flagExclude))) {\n                    continue;\n                }\n                if (tagFilter) {\n                    const val = record.get(tagFilter.tag);\n                    if (!(val === '*' ? val !== undefined : val === tagFilter.value)) {\n                        continue;\n                    }\n                }\n                if (readName && record.get('name') !== readName) {\n                    continue;\n                }\n                observer.next(new BamSlightlyLazyFeature(record, this, ref));\n            }\n            statusCallback('');\n            observer.complete();\n        }, signal);\n    }\n    async estimateRegionsStats(regions, opts) {\n        const { bam } = await this.configure();\n        // this is a method to avoid calling on htsget adapters\n        // @ts-ignore\n        if (bam.index.filehandle !== '?') {\n            const bytes = await bytesForRegions(regions, bam);\n            const fetchSizeLimit = readConfObject(this.config, 'fetchSizeLimit');\n            return { bytes, fetchSizeLimit };\n        }\n        else {\n            return super.estimateRegionsStats(regions, opts);\n        }\n    }\n    freeResources( /* { region } */) { }\n    // depends on setup being called before the BAM constructor\n    refIdToName(refId) {\n        var _a;\n        return (_a = this.samHeader) === null || _a === void 0 ? void 0 : _a.idToName[refId];\n    }\n}\n//# sourceMappingURL=BamAdapter.js.map","import { getMismatches } from './MismatchParser';\nexport default class BamSlightlyLazyFeature {\n    // uses parameter properties to automatically create fields on the class\n    // https://www.typescriptlang.org/docs/handbook/classes.html#parameter-properties\n    constructor(record, adapter, ref) {\n        this.record = record;\n        this.adapter = adapter;\n        this.ref = ref;\n    }\n    _get_name() {\n        return this.record.get('name');\n    }\n    _get_type() {\n        return 'match';\n    }\n    _get_score() {\n        return this.record.get('mq');\n    }\n    _get_flags() {\n        return this.record.flags;\n    }\n    _get_strand() {\n        return this.record.isReverseComplemented() ? -1 : 1;\n    }\n    _get_pair_orientation() {\n        return this.record.isPaired() ? this.record.getPairOrientation() : undefined;\n    }\n    _get_next_seq_id() {\n        return this.record._next_refid();\n    }\n    _get_seq_id() {\n        // @ts-ignore\n        return this.record._refID;\n    }\n    _get_next_refName() {\n        return this.adapter.refIdToName(this.record._next_refid());\n    }\n    _get_next_segment_position() {\n        const { record, adapter } = this;\n        return record.isPaired()\n            ? `${adapter.refIdToName(record._next_refid())}:${record._next_pos() + 1}`\n            : undefined;\n    }\n    _get_seq() {\n        return this.record.getReadBases();\n    }\n    qualRaw() {\n        return this.record.qualRaw();\n    }\n    set() { }\n    tags() {\n        const properties = Object.getOwnPropertyNames(BamSlightlyLazyFeature.prototype);\n        return [\n            ...new Set(properties\n                .filter(prop => prop.startsWith('_get_') &&\n                prop !== '_get_mismatches' &&\n                prop !== '_get_tags' &&\n                prop !== '_get_next_seq_id' &&\n                prop !== '_get_seq_id')\n                .map(methodName => methodName.replace('_get_', ''))\n                .concat(this.record._tags())),\n        ];\n    }\n    id() {\n        return `${this.adapter.id}-${this.record.id()}`;\n    }\n    // eslint-disable-next-line @typescript-eslint/no-explicit-any\n    get(field) {\n        const methodName = `_get_${field}`;\n        // @ts-ignore\n        if (this[methodName]) {\n            // @ts-ignore\n            return this[methodName]();\n        }\n        return this.record.get(field);\n    }\n    _get_refName() {\n        return this.adapter.refIdToName(this.record.seq_id());\n    }\n    parent() {\n        return undefined;\n    }\n    children() {\n        return undefined;\n    }\n    pairedFeature() {\n        return false;\n    }\n    toJSON() {\n        return {\n            ...Object.fromEntries(this.tags()\n                .map(t => [t, this.get(t)])\n                .filter(elt => elt[1] !== undefined)),\n            uniqueId: this.id(),\n        };\n    }\n    _get_mismatches() {\n        return getMismatches(this.get('CIGAR'), this.get('MD'), this.get('seq'), this.ref, this.qualRaw());\n    }\n    _get_clipPos() {\n        const cigar = this.get('CIGAR') || '';\n        return this.get('strand') === -1\n            ? +(cigar.match(/(\\d+)[SH]$/) || [])[1] || 0\n            : +(cigar.match(/^(\\d+)([SH])/) || [])[1] || 0;\n    }\n}\n//# sourceMappingURL=BamSlightlyLazyFeature.js.map","(function(self) {\n\nvar irrelevant = (function (exports) {\n\n  var support = {\n    searchParams: 'URLSearchParams' in self,\n    iterable: 'Symbol' in self && 'iterator' in Symbol,\n    blob:\n      'FileReader' in self &&\n      'Blob' in self &&\n      (function() {\n        try {\n          new Blob();\n          return true\n        } catch (e) {\n          return false\n        }\n      })(),\n    formData: 'FormData' in self,\n    arrayBuffer: 'ArrayBuffer' in self\n  };\n\n  function isDataView(obj) {\n    return obj && DataView.prototype.isPrototypeOf(obj)\n  }\n\n  if (support.arrayBuffer) {\n    var viewClasses = [\n      '[object Int8Array]',\n      '[object Uint8Array]',\n      '[object Uint8ClampedArray]',\n      '[object Int16Array]',\n      '[object Uint16Array]',\n      '[object Int32Array]',\n      '[object Uint32Array]',\n      '[object Float32Array]',\n      '[object Float64Array]'\n    ];\n\n    var isArrayBufferView =\n      ArrayBuffer.isView ||\n      function(obj) {\n        return obj && viewClasses.indexOf(Object.prototype.toString.call(obj)) > -1\n      };\n  }\n\n  function normalizeName(name) {\n    if (typeof name !== 'string') {\n      name = String(name);\n    }\n    if (/[^a-z0-9\\-#$%&'*+.^_`|~]/i.test(name)) {\n      throw new TypeError('Invalid character in header field name')\n    }\n    return name.toLowerCase()\n  }\n\n  function normalizeValue(value) {\n    if (typeof value !== 'string') {\n      value = String(value);\n    }\n    return value\n  }\n\n  // Build a destructive iterator for the value list\n  function iteratorFor(items) {\n    var iterator = {\n      next: function() {\n        var value = items.shift();\n        return {done: value === undefined, value: value}\n      }\n    };\n\n    if (support.iterable) {\n      iterator[Symbol.iterator] = function() {\n        return iterator\n      };\n    }\n\n    return iterator\n  }\n\n  function Headers(headers) {\n    this.map = {};\n\n    if (headers instanceof Headers) {\n      headers.forEach(function(value, name) {\n        this.append(name, value);\n      }, this);\n    } else if (Array.isArray(headers)) {\n      headers.forEach(function(header) {\n        this.append(header[0], header[1]);\n      }, this);\n    } else if (headers) {\n      Object.getOwnPropertyNames(headers).forEach(function(name) {\n        this.append(name, headers[name]);\n      }, this);\n    }\n  }\n\n  Headers.prototype.append = function(name, value) {\n    name = normalizeName(name);\n    value = normalizeValue(value);\n    var oldValue = this.map[name];\n    this.map[name] = oldValue ? oldValue + ', ' + value : value;\n  };\n\n  Headers.prototype['delete'] = function(name) {\n    delete this.map[normalizeName(name)];\n  };\n\n  Headers.prototype.get = function(name) {\n    name = normalizeName(name);\n    return this.has(name) ? this.map[name] : null\n  };\n\n  Headers.prototype.has = function(name) {\n    return this.map.hasOwnProperty(normalizeName(name))\n  };\n\n  Headers.prototype.set = function(name, value) {\n    this.map[normalizeName(name)] = normalizeValue(value);\n  };\n\n  Headers.prototype.forEach = function(callback, thisArg) {\n    for (var name in this.map) {\n      if (this.map.hasOwnProperty(name)) {\n        callback.call(thisArg, this.map[name], name, this);\n      }\n    }\n  };\n\n  Headers.prototype.keys = function() {\n    var items = [];\n    this.forEach(function(value, name) {\n      items.push(name);\n    });\n    return iteratorFor(items)\n  };\n\n  Headers.prototype.values = function() {\n    var items = [];\n    this.forEach(function(value) {\n      items.push(value);\n    });\n    return iteratorFor(items)\n  };\n\n  Headers.prototype.entries = function() {\n    var items = [];\n    this.forEach(function(value, name) {\n      items.push([name, value]);\n    });\n    return iteratorFor(items)\n  };\n\n  if (support.iterable) {\n    Headers.prototype[Symbol.iterator] = Headers.prototype.entries;\n  }\n\n  function consumed(body) {\n    if (body.bodyUsed) {\n      return Promise.reject(new TypeError('Already read'))\n    }\n    body.bodyUsed = true;\n  }\n\n  function fileReaderReady(reader) {\n    return new Promise(function(resolve, reject) {\n      reader.onload = function() {\n        resolve(reader.result);\n      };\n      reader.onerror = function() {\n        reject(reader.error);\n      };\n    })\n  }\n\n  function readBlobAsArrayBuffer(blob) {\n    var reader = new FileReader();\n    var promise = fileReaderReady(reader);\n    reader.readAsArrayBuffer(blob);\n    return promise\n  }\n\n  function readBlobAsText(blob) {\n    var reader = new FileReader();\n    var promise = fileReaderReady(reader);\n    reader.readAsText(blob);\n    return promise\n  }\n\n  function readArrayBufferAsText(buf) {\n    var view = new Uint8Array(buf);\n    var chars = new Array(view.length);\n\n    for (var i = 0; i < view.length; i++) {\n      chars[i] = String.fromCharCode(view[i]);\n    }\n    return chars.join('')\n  }\n\n  function bufferClone(buf) {\n    if (buf.slice) {\n      return buf.slice(0)\n    } else {\n      var view = new Uint8Array(buf.byteLength);\n      view.set(new Uint8Array(buf));\n      return view.buffer\n    }\n  }\n\n  function Body() {\n    this.bodyUsed = false;\n\n    this._initBody = function(body) {\n      this._bodyInit = body;\n      if (!body) {\n        this._bodyText = '';\n      } else if (typeof body === 'string') {\n        this._bodyText = body;\n      } else if (support.blob && Blob.prototype.isPrototypeOf(body)) {\n        this._bodyBlob = body;\n      } else if (support.formData && FormData.prototype.isPrototypeOf(body)) {\n        this._bodyFormData = body;\n      } else if (support.searchParams && URLSearchParams.prototype.isPrototypeOf(body)) {\n        this._bodyText = body.toString();\n      } else if (support.arrayBuffer && support.blob && isDataView(body)) {\n        this._bodyArrayBuffer = bufferClone(body.buffer);\n        // IE 10-11 can't handle a DataView body.\n        this._bodyInit = new Blob([this._bodyArrayBuffer]);\n      } else if (support.arrayBuffer && (ArrayBuffer.prototype.isPrototypeOf(body) || isArrayBufferView(body))) {\n        this._bodyArrayBuffer = bufferClone(body);\n      } else {\n        this._bodyText = body = Object.prototype.toString.call(body);\n      }\n\n      if (!this.headers.get('content-type')) {\n        if (typeof body === 'string') {\n          this.headers.set('content-type', 'text/plain;charset=UTF-8');\n        } else if (this._bodyBlob && this._bodyBlob.type) {\n          this.headers.set('content-type', this._bodyBlob.type);\n        } else if (support.searchParams && URLSearchParams.prototype.isPrototypeOf(body)) {\n          this.headers.set('content-type', 'application/x-www-form-urlencoded;charset=UTF-8');\n        }\n      }\n    };\n\n    if (support.blob) {\n      this.blob = function() {\n        var rejected = consumed(this);\n        if (rejected) {\n          return rejected\n        }\n\n        if (this._bodyBlob) {\n          return Promise.resolve(this._bodyBlob)\n        } else if (this._bodyArrayBuffer) {\n          return Promise.resolve(new Blob([this._bodyArrayBuffer]))\n        } else if (this._bodyFormData) {\n          throw new Error('could not read FormData body as blob')\n        } else {\n          return Promise.resolve(new Blob([this._bodyText]))\n        }\n      };\n\n      this.arrayBuffer = function() {\n        if (this._bodyArrayBuffer) {\n          return consumed(this) || Promise.resolve(this._bodyArrayBuffer)\n        } else {\n          return this.blob().then(readBlobAsArrayBuffer)\n        }\n      };\n    }\n\n    this.text = function() {\n      var rejected = consumed(this);\n      if (rejected) {\n        return rejected\n      }\n\n      if (this._bodyBlob) {\n        return readBlobAsText(this._bodyBlob)\n      } else if (this._bodyArrayBuffer) {\n        return Promise.resolve(readArrayBufferAsText(this._bodyArrayBuffer))\n      } else if (this._bodyFormData) {\n        throw new Error('could not read FormData body as text')\n      } else {\n        return Promise.resolve(this._bodyText)\n      }\n    };\n\n    if (support.formData) {\n      this.formData = function() {\n        return this.text().then(decode)\n      };\n    }\n\n    this.json = function() {\n      return this.text().then(JSON.parse)\n    };\n\n    return this\n  }\n\n  // HTTP methods whose capitalization should be normalized\n  var methods = ['DELETE', 'GET', 'HEAD', 'OPTIONS', 'POST', 'PUT'];\n\n  function normalizeMethod(method) {\n    var upcased = method.toUpperCase();\n    return methods.indexOf(upcased) > -1 ? upcased : method\n  }\n\n  function Request(input, options) {\n    options = options || {};\n    var body = options.body;\n\n    if (input instanceof Request) {\n      if (input.bodyUsed) {\n        throw new TypeError('Already read')\n      }\n      this.url = input.url;\n      this.credentials = input.credentials;\n      if (!options.headers) {\n        this.headers = new Headers(input.headers);\n      }\n      this.method = input.method;\n      this.mode = input.mode;\n      this.signal = input.signal;\n      if (!body && input._bodyInit != null) {\n        body = input._bodyInit;\n        input.bodyUsed = true;\n      }\n    } else {\n      this.url = String(input);\n    }\n\n    this.credentials = options.credentials || this.credentials || 'same-origin';\n    if (options.headers || !this.headers) {\n      this.headers = new Headers(options.headers);\n    }\n    this.method = normalizeMethod(options.method || this.method || 'GET');\n    this.mode = options.mode || this.mode || null;\n    this.signal = options.signal || this.signal;\n    this.referrer = null;\n\n    if ((this.method === 'GET' || this.method === 'HEAD') && body) {\n      throw new TypeError('Body not allowed for GET or HEAD requests')\n    }\n    this._initBody(body);\n  }\n\n  Request.prototype.clone = function() {\n    return new Request(this, {body: this._bodyInit})\n  };\n\n  function decode(body) {\n    var form = new FormData();\n    body\n      .trim()\n      .split('&')\n      .forEach(function(bytes) {\n        if (bytes) {\n          var split = bytes.split('=');\n          var name = split.shift().replace(/\\+/g, ' ');\n          var value = split.join('=').replace(/\\+/g, ' ');\n          form.append(decodeURIComponent(name), decodeURIComponent(value));\n        }\n      });\n    return form\n  }\n\n  function parseHeaders(rawHeaders) {\n    var headers = new Headers();\n    // Replace instances of \\r\\n and \\n followed by at least one space or horizontal tab with a space\n    // https://tools.ietf.org/html/rfc7230#section-3.2\n    var preProcessedHeaders = rawHeaders.replace(/\\r?\\n[\\t ]+/g, ' ');\n    preProcessedHeaders.split(/\\r?\\n/).forEach(function(line) {\n      var parts = line.split(':');\n      var key = parts.shift().trim();\n      if (key) {\n        var value = parts.join(':').trim();\n        headers.append(key, value);\n      }\n    });\n    return headers\n  }\n\n  Body.call(Request.prototype);\n\n  function Response(bodyInit, options) {\n    if (!options) {\n      options = {};\n    }\n\n    this.type = 'default';\n    this.status = options.status === undefined ? 200 : options.status;\n    this.ok = this.status >= 200 && this.status < 300;\n    this.statusText = 'statusText' in options ? options.statusText : 'OK';\n    this.headers = new Headers(options.headers);\n    this.url = options.url || '';\n    this._initBody(bodyInit);\n  }\n\n  Body.call(Response.prototype);\n\n  Response.prototype.clone = function() {\n    return new Response(this._bodyInit, {\n      status: this.status,\n      statusText: this.statusText,\n      headers: new Headers(this.headers),\n      url: this.url\n    })\n  };\n\n  Response.error = function() {\n    var response = new Response(null, {status: 0, statusText: ''});\n    response.type = 'error';\n    return response\n  };\n\n  var redirectStatuses = [301, 302, 303, 307, 308];\n\n  Response.redirect = function(url, status) {\n    if (redirectStatuses.indexOf(status) === -1) {\n      throw new RangeError('Invalid status code')\n    }\n\n    return new Response(null, {status: status, headers: {location: url}})\n  };\n\n  exports.DOMException = self.DOMException;\n  try {\n    new exports.DOMException();\n  } catch (err) {\n    exports.DOMException = function(message, name) {\n      this.message = message;\n      this.name = name;\n      var error = Error(message);\n      this.stack = error.stack;\n    };\n    exports.DOMException.prototype = Object.create(Error.prototype);\n    exports.DOMException.prototype.constructor = exports.DOMException;\n  }\n\n  function fetch(input, init) {\n    return new Promise(function(resolve, reject) {\n      var request = new Request(input, init);\n\n      if (request.signal && request.signal.aborted) {\n        return reject(new exports.DOMException('Aborted', 'AbortError'))\n      }\n\n      var xhr = new XMLHttpRequest();\n\n      function abortXhr() {\n        xhr.abort();\n      }\n\n      xhr.onload = function() {\n        var options = {\n          status: xhr.status,\n          statusText: xhr.statusText,\n          headers: parseHeaders(xhr.getAllResponseHeaders() || '')\n        };\n        options.url = 'responseURL' in xhr ? xhr.responseURL : options.headers.get('X-Request-URL');\n        var body = 'response' in xhr ? xhr.response : xhr.responseText;\n        resolve(new Response(body, options));\n      };\n\n      xhr.onerror = function() {\n        reject(new TypeError('Network request failed'));\n      };\n\n      xhr.ontimeout = function() {\n        reject(new TypeError('Network request failed'));\n      };\n\n      xhr.onabort = function() {\n        reject(new exports.DOMException('Aborted', 'AbortError'));\n      };\n\n      xhr.open(request.method, request.url, true);\n\n      if (request.credentials === 'include') {\n        xhr.withCredentials = true;\n      } else if (request.credentials === 'omit') {\n        xhr.withCredentials = false;\n      }\n\n      if ('responseType' in xhr && support.blob) {\n        xhr.responseType = 'blob';\n      }\n\n      request.headers.forEach(function(value, name) {\n        xhr.setRequestHeader(name, value);\n      });\n\n      if (request.signal) {\n        request.signal.addEventListener('abort', abortXhr);\n\n        xhr.onreadystatechange = function() {\n          // DONE (success or failure)\n          if (xhr.readyState === 4) {\n            request.signal.removeEventListener('abort', abortXhr);\n          }\n        };\n      }\n\n      xhr.send(typeof request._bodyInit === 'undefined' ? null : request._bodyInit);\n    })\n  }\n\n  fetch.polyfill = true;\n\n  if (!self.fetch) {\n    self.fetch = fetch;\n    self.Headers = Headers;\n    self.Request = Request;\n    self.Response = Response;\n  }\n\n  exports.Headers = Headers;\n  exports.Request = Request;\n  exports.Response = Response;\n  exports.fetch = fetch;\n\n  Object.defineProperty(exports, '__esModule', { value: true });\n\n  return exports;\n\n})({});\n})(typeof self !== 'undefined' ? self : this);\n"],"names":[],"sourceRoot":""}