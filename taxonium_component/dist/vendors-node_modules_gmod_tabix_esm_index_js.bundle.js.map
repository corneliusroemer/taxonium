{"version":3,"file":"vendors-node_modules_gmod_tabix_esm_index_js.bundle.js","mappings":";;;;;;;;;;;;;AAAA;AACe;AACf;AACA,eAAe,eAAe;AAC9B,eAAe,eAAe;AAC9B,eAAe,QAAQ;AACvB,eAAe,QAAQ;AACvB;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,kBAAkB,UAAU,IAAI,WAAW,OAAO,SAAS,gBAAgB,mBAAmB;AAC9F;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;;;;;;;;;;;;;;;;;;;;AChCwB;AACsB;AACa;AAC/B;AAC0B;AAClB;AACpC,6BAA6B;AAC7B,6BAA6B;AAC7B;AACA;AACA;AACA;AACA;AACA;AACe,kBAAkB,kDAAS;AAC1C;AACA;AACA;AACA;AACA;AACA;AACA,sCAAsC;AACtC;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,gBAAgB,QAAQ;AACxB;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,+BAA+B;AAC/B;AACA;AACA;AACA;AACA,yBAAyB,kCAAkC;AAC3D;AACA,iEAAiE,YAAY;AAC7E;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,gBAAgB,2BAA2B;AAC3C;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,wBAAwB,uBAAuB;AAC/C;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,iBAAiB;AACjB;AACA;AACA,0BAA0B;AAC1B,4BAA4B,4DAAK;AACjC;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,2BAA2B;AAC3B;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,uBAAuB;AACvB,4BAA4B,cAAc;AAC1C;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,oCAAoC,yDAAS;AAC7C;AACA;AACA;AACA;AACA,oCAAoC,gBAAgB;AACpD,kCAAkC,yDAAS;AAC3C,kCAAkC,yDAAS;AAC3C;AACA;AACA,wCAAwC,8CAAK;AAC7C;AACA;AACA;AACA;AACA,qBAAqB;AACrB,SAAS;AACT;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,0BAA0B,mDAAY,CAAC,uDAAgB;AACvD,iBAAiB;AACjB;AACA,qDAAqD;AACrD;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,mBAAmB,wBAAwB;AAC3C,yDAAyD;AACzD;AACA;AACA;AACA,kCAAkC,YAAY;AAC9C;AACA;AACA,oCAAoC,sBAAsB;AAC1D,wCAAwC,8CAAK;AAC7C;AACA;AACA;AACA;AACA,eAAe,qDAAc,aAAa,sDAAa;AACvD;AACA;AACA;AACA;AACA;AACA,kBAAkB;AAClB;AACA;AACA;AACA;AACA;AACA,UAAU;AACV;AACA;AACA;AACA;AACA;AACA,eAAe,iBAAiB;AAChC;AACA;AACA;AACA,yCAAyC,IAAI,GAAG,KAAK,iDAAiD,cAAc,UAAU,WAAW;AACzI;AACA;AACA;AACA;AACA;AACA;AACA;;;;;;;;;;;;;;;;;;;ACrOkD;AAC1B;AACA;AACc;AACtC;;;;;;;;;;;;;;;;;;ACJ4D;AAC3B;AAClB;AACf;AACA,eAAe,YAAY;AAC3B,eAAe,UAAU;AACzB;AACA,kBAAkB,uCAAuC;AACzD;AACA;AACA;AACA,+BAA+B;AAC/B;AACA,gBAAgB,mBAAmB;AACnC;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,yBAAyB;AACzB;AACA,mCAAmC,gEAAqB;AACxD,2BAA2B,kDAAQ,GAAG,YAAY;AAClD;AACA,aAAa;AACb;AACA;AACA;AACA,oCAAoC;AACpC,+DAA+D;AAC/D;AACA;AACA;;;;;;;;;;;;;;;;;;;;;;;;ACvC4D;AAChC;AACmB;AACgB;AACrB;AAClB;AACA;AACxB;AACA;AACA;AACA,KAAK;AACL;AACe;AACf;AACA,eAAe,QAAQ;AACvB,eAAe,QAAQ;AACvB,eAAe,YAAY;AAC3B,eAAe,QAAQ;AACvB,eAAe,YAAY;AAC3B,eAAe,QAAQ;AACvB,eAAe,YAAY;AAC3B,eAAe,gBAAgB;AAC/B,eAAe,UAAU;AACzB;AACA;AACA,eAAe,QAAQ;AACvB,eAAe,QAAQ;AACvB;AACA,kBAAkB,oJAAoJ;AACtK;AACA;AACA;AACA;AACA,kCAAkC,yDAAS;AAC3C;AACA;AACA;AACA;AACA;AACA,6BAA6B,4CAAG;AAChC;AACA;AACA,aAAa;AACb;AACA;AACA,6BAA6B,4CAAG;AAChC;AACA;AACA,aAAa;AACb;AACA;AACA,6BAA6B,4CAAG;AAChC,gCAAgC,yDAAS;AACzC;AACA,aAAa;AACb;AACA;AACA,6BAA6B,4CAAG;AAChC,gCAAgC,yDAAS;AACzC;AACA,aAAa;AACb;AACA;AACA,6BAA6B,4CAAG;AAChC,gCAAgC,yDAAS,IAAI,KAAK;AAClD;AACA,aAAa;AACb;AACA;AACA;AACA;AACA;AACA;AACA,8BAA8B,gEAAqB;AACnD,uBAAuB,kDAAG;AAC1B;AACA,aAAa;AACb;AACA,SAAS;AACT;AACA;AACA,eAAe,QAAQ;AACvB,eAAe,QAAQ;AACvB,eAAe,QAAQ;AACvB,eAAe,iBAAiB;AAChC,iBAAiB,SAAS;AAC1B;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,QAAQ,uDAAgB;AACxB;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,QAAQ,uDAAgB;AACxB;AACA;AACA,wBAAwB,mBAAmB;AAC3C;AACA;AACA,6DAA6D,uBAAuB,kCAAkC,qCAAqC;AAC3J;AACA;AACA;AACA;AACA,+BAA+B,0BAA0B;AACzD;AACA;AACA,oBAAoB,iCAAiC;AACrD;AACA;AACA;AACA;AACA,YAAY,uDAAgB;AAC5B;AACA;AACA,4BAA4B,kBAAkB;AAC9C;AACA,8BAA8B,+BAA+B;AAC7D;AACA,wBAAwB,4BAA4B;AACpD;AACA;AACA;AACA;AACA,6EAA6E,yBAAyB,IAAI,gBAAgB;AAC1H;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,oBAAoB,uDAAgB;AACpC;AACA;AACA;AACA;AACA;AACA,+BAA+B;AAC/B;AACA;AACA;AACA;AACA;AACA;AACA;AACA,iBAAiB,SAAS;AAC1B;AACA,mCAAmC;AACnC,gBAAgB,wCAAwC;AACxD,QAAQ,uDAAgB;AACxB;AACA;AACA;AACA;AACA;AACA;AACA,QAAQ,uDAAgB;AACxB;AACA,0BAA0B,4DAAK;AAC/B;AACA;AACA;AACA;AACA;AACA,yCAAyC,QAAQ,eAAe,SAAS,IAAI,EAAE;AAC/E;AACA;AACA;AACA;AACA;AACA;AACA;AACA,4BAA4B,kBAAkB;AAC9C;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,iBAAiB,SAAS;AAC1B;AACA,6BAA6B;AAC7B;AACA,QAAQ,uDAAgB;AACxB;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,iBAAiB,SAAS;AAC1B;AACA,6CAA6C;AAC7C;AACA;AACA;AACA;AACA,eAAe,QAAQ;AACvB;AACA,eAAe,QAAQ;AACvB,eAAe,QAAQ;AACvB,eAAe,QAAQ;AACvB,eAAe,eAAe;AAC9B,iBAAiB,QAAQ,OAAO,0BAA0B;AAC1D;AACA;AACA,gBAAgB,kDAAkD;AAClE;AACA;AACA,qBAAqB;AACrB;AACA;AACA,cAAc,kBAAkB;AAChC;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,qCAAqC;AACrC;AACA;AACA;AACA,wBAAwB,qBAAqB;AAC7C;AACA;AACA;AACA;AACA,iCAAiC;AACjC;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,iCAAiC;AACjC;AACA;AACA;AACA;AACA,qCAAqC;AACrC;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,iCAAiC;AACjC;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,iBAAiB;AACjB;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,6BAA6B;AAC7B,4BAA4B,iBAAiB;AAC7C,mCAAmC;AACnC,kDAAkD;AAClD;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,eAAe,QAAQ;AACvB,iBAAiB,SAAS;AAC1B;AACA,sCAAsC;AACtC;AACA;AACA,yDAAyD;AACzD,gBAAgB,oBAAoB,6BAA6B,MAAM;AACvE;AACA;AACA;AACA;AACA;AACA,eAAe,OAAO;AACtB,iBAAiB,SAAS;AAC1B;AACA,oCAAoC;AACpC;AACA;AACA;AACA;AACA,mBAAmB,sEAAe;AAClC;AACA;AACA,yDAAyD,kBAAkB,EAAE,EAAE;AAC/E;AACA;AACA;AACA;;;;;;;;;;;;;;;;;;;;;AC7YwB;AACmC;AAC/B;AACkB;AAC0B;AACpC;AACpC,4BAA4B;AAC5B;AACA;AACA;AACA;AACA;AACA,cAAc;AACd;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACe,yBAAyB,kDAAS;AACjD,sCAAsC;AACtC;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,gBAAgB,QAAQ;AACxB;AACA;AACA;AACA;AACA;AACA;AACA;AACA,0BAA0B;AAC1B,4BAA4B,4DAAK;AACjC,QAAQ,uDAAgB;AACxB;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,iEAAiE,YAAY;AAC7E;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,gBAAgB,2BAA2B;AAC3C;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,4BAA4B,cAAc;AAC1C;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,oCAAoC,gBAAgB;AACpD,kCAAkC,yDAAS;AAC3C,kCAAkC,yDAAS;AAC3C;AACA;AACA,wCAAwC,8CAAK;AAC7C;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,4BAA4B,iBAAiB;AAC7C,iCAAiC,yDAAS;AAC1C;AACA;AACA;AACA,qBAAqB;AACrB,SAAS;AACT;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,0BAA0B,mDAAY,CAAC,uDAAgB;AACvD,iBAAiB;AACjB;AACA;AACA;AACA;AACA;AACA;AACA,wBAAwB,uBAAuB;AAC/C;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,iBAAiB;AACjB;AACA,qDAAqD;AACrD;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,kBAAkB,sDAAa;AAC/B;AACA;AACA;AACA,mBAAmB,wBAAwB;AAC3C,oDAAoD;AACpD;AACA;AACA;AACA,kCAAkC,YAAY;AAC9C;AACA;AACA,oCAAoC,sBAAsB;AAC1D,wCAAwC,8CAAK;AAC7C;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,6BAA6B,aAAa;AAC1C;AACA;AACA;AACA;AACA;AACA;AACA;AACA,eAAe,qDAAc;AAC7B;AACA;AACA;;;;;;;;;;;;;;;;;;ACvNO;AACP;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,WAAW,aAAa;AACxB;AACA;AACO;AACP;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,WAAW,aAAa;AACxB;AACO;AACP;AACA;AACA;AACO;AACP;AACA;AACA;AACO;AACP;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,KAAK;AACL;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,2CAA2C,MAAM;AACjD;AACA,KAAK;AACL;AACA;AACA;;;;;;;;;;;;;;;AC1Fe;AACf;AACA,4CAA4C;AAC5C,0CAA0C;AAC1C;AACA;AACA,kBAAkB,mBAAmB,GAAG,kBAAkB;AAC1D;AACA;AACA;AACA;AACA;AACA;AACA;AACA,eAAe,MAAM;AACrB;AACA;AACA,eAAe,iBAAiB;AAChC;AACA;AACA;AACA;AACA;AACA;AACA;AACO;AACP;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;;;;;;;;;ACpCa;;AAEb;AACA,yBAAyB;AACzB;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA,IAAI;AACJ;AACA;;AAEA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;;AAEA;AACA;;AAEA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;;AAEA","sources":["webpack://taxonium/./node_modules/@gmod/tabix/esm/chunk.js","webpack://taxonium/./node_modules/@gmod/tabix/esm/csi.js","webpack://taxonium/./node_modules/@gmod/tabix/esm/index.js","webpack://taxonium/./node_modules/@gmod/tabix/esm/indexFile.js","webpack://taxonium/./node_modules/@gmod/tabix/esm/tabixIndexedFile.js","webpack://taxonium/./node_modules/@gmod/tabix/esm/tbi.js","webpack://taxonium/./node_modules/@gmod/tabix/esm/util.js","webpack://taxonium/./node_modules/@gmod/tabix/esm/virtualOffset.js","webpack://taxonium/./node_modules/@gmod/tabix/node_modules/quick-lru/index.js"],"sourcesContent":["// little class representing a chunk in the index\nexport default class Chunk {\n    /**\n     * @param {VirtualOffset} minv\n     * @param {VirtualOffset} maxv\n     * @param {number} bin\n     * @param {number} [fetchedSize]\n     */\n    constructor(minv, maxv, bin, fetchedSize = undefined) {\n        this.minv = minv;\n        this.maxv = maxv;\n        this.bin = bin;\n        this._fetchedSize = fetchedSize;\n    }\n    toUniqueString() {\n        return `${this.minv}..${this.maxv} (bin ${this.bin}, fetchedSize ${this.fetchedSize()})`;\n    }\n    toString() {\n        return this.toUniqueString();\n    }\n    compareTo(b) {\n        return (this.minv.compareTo(b.minv) ||\n            this.maxv.compareTo(b.maxv) ||\n            this.bin - b.bin);\n    }\n    fetchedSize() {\n        if (this._fetchedSize !== undefined) {\n            return this._fetchedSize;\n        }\n        return this.maxv.blockPosition + (1 << 16) - this.minv.blockPosition;\n    }\n}\n//# sourceMappingURL=chunk.js.map","import Long from 'long';\nimport { unzip } from '@gmod/bgzf-filehandle';\nimport VirtualOffset, { fromBytes } from './virtualOffset';\nimport Chunk from './chunk';\nimport { longToNumber, optimizeChunks } from './util';\nimport IndexFile from './indexFile';\nconst CSI1_MAGIC = 21582659; // CSI\\1\nconst CSI2_MAGIC = 38359875; // CSI\\2\nfunction lshift(num, bits) {\n    return num * 2 ** bits;\n}\nfunction rshift(num, bits) {\n    return Math.floor(num / 2 ** bits);\n}\nexport default class CSI extends IndexFile {\n    constructor(args) {\n        super(args);\n        this.maxBinNumber = 0;\n        this.depth = 0;\n        this.minShift = 0;\n    }\n    async lineCount(refName, opts = {}) {\n        const indexData = await this.parse(opts);\n        if (!indexData) {\n            return -1;\n        }\n        const refId = indexData.refNameToId[refName];\n        const idx = indexData.indices[refId];\n        if (!idx) {\n            return -1;\n        }\n        const { stats } = indexData.indices[refId];\n        if (stats) {\n            return stats.lineCount;\n        }\n        return -1;\n    }\n    async indexCov() {\n        throw new Error('CSI indexes do not support indexcov');\n        return [];\n    }\n    parseAuxData(bytes, offset, auxLength) {\n        if (auxLength < 30) {\n            return {\n                refIdToName: [],\n                refNameToId: {},\n            };\n        }\n        const formatFlags = bytes.readInt32LE(offset);\n        const coordinateType = formatFlags & 0x10000 ? 'zero-based-half-open' : '1-based-closed';\n        const format = { 0: 'generic', 1: 'SAM', 2: 'VCF' }[formatFlags & 0xf];\n        if (!format) {\n            throw new Error(`invalid Tabix preset format flags ${formatFlags}`);\n        }\n        const columnNumbers = {\n            ref: bytes.readInt32LE(offset + 4),\n            start: bytes.readInt32LE(offset + 8),\n            end: bytes.readInt32LE(offset + 12),\n        };\n        const metaValue = bytes.readInt32LE(offset + 16);\n        const metaChar = metaValue ? String.fromCharCode(metaValue) : '';\n        const skipLines = bytes.readInt32LE(offset + 20);\n        const nameSectionLength = bytes.readInt32LE(offset + 24);\n        const { refIdToName, refNameToId } = this._parseNameBytes(bytes.slice(offset + 28, offset + 28 + nameSectionLength));\n        return {\n            refIdToName,\n            refNameToId,\n            skipLines,\n            metaChar,\n            columnNumbers,\n            format,\n            coordinateType,\n        };\n    }\n    _parseNameBytes(namesBytes) {\n        let currRefId = 0;\n        let currNameStart = 0;\n        const refIdToName = [];\n        const refNameToId = {};\n        for (let i = 0; i < namesBytes.length; i += 1) {\n            if (!namesBytes[i]) {\n                if (currNameStart < i) {\n                    let refName = namesBytes.toString('utf8', currNameStart, i);\n                    refName = this.renameRefSeq(refName);\n                    refIdToName[currRefId] = refName;\n                    refNameToId[refName] = currRefId;\n                }\n                currNameStart = i + 1;\n                currRefId += 1;\n            }\n        }\n        return { refNameToId, refIdToName };\n    }\n    // fetch and parse the index\n    async _parse(opts = {}) {\n        const bytes = await unzip((await this.filehandle.readFile(opts)));\n        // check TBI magic numbers\n        let csiVersion;\n        if (bytes.readUInt32LE(0) === CSI1_MAGIC) {\n            csiVersion = 1;\n        }\n        else if (bytes.readUInt32LE(0) === CSI2_MAGIC) {\n            csiVersion = 2;\n        }\n        else {\n            throw new Error('Not a CSI file');\n            // TODO: do we need to support big-endian CSI files?\n        }\n        this.minShift = bytes.readInt32LE(4);\n        this.depth = bytes.readInt32LE(8);\n        this.maxBinNumber = ((1 << ((this.depth + 1) * 3)) - 1) / 7;\n        const maxRefLength = 2 ** (this.minShift + this.depth * 3);\n        const auxLength = bytes.readInt32LE(12);\n        let aux = {\n            refIdToName: [],\n            refNameToId: {},\n        };\n        if (auxLength) {\n            aux = this.parseAuxData(bytes, 16, auxLength);\n        }\n        const refCount = bytes.readInt32LE(16 + auxLength);\n        // read the indexes for each reference sequence\n        let firstDataLine;\n        let currOffset = 16 + auxLength + 4;\n        const indices = new Array(refCount).fill(0).map(() => {\n            // the binning index\n            const binCount = bytes.readInt32LE(currOffset);\n            currOffset += 4;\n            const binIndex = {};\n            let stats; // < provided by parsing a pseudo-bin, if present\n            for (let j = 0; j < binCount; j += 1) {\n                const bin = bytes.readUInt32LE(currOffset);\n                if (bin > this.maxBinNumber) {\n                    // this is a fake bin that actually has stats information\n                    // about the reference sequence in it\n                    stats = this.parsePseudoBin(bytes, currOffset + 4);\n                    currOffset += 4 + 8 + 4 + 16 + 16;\n                }\n                else {\n                    const loffset = fromBytes(bytes, currOffset + 4);\n                    firstDataLine = this._findFirstData(firstDataLine, loffset);\n                    const chunkCount = bytes.readInt32LE(currOffset + 12);\n                    currOffset += 16;\n                    const chunks = new Array(chunkCount);\n                    for (let k = 0; k < chunkCount; k += 1) {\n                        const u = fromBytes(bytes, currOffset);\n                        const v = fromBytes(bytes, currOffset + 8);\n                        currOffset += 16;\n                        // this._findFirstData(data, u)\n                        chunks[k] = new Chunk(u, v, bin);\n                    }\n                    binIndex[bin] = chunks;\n                }\n            }\n            return { binIndex, stats };\n        });\n        return {\n            ...aux,\n            csi: true,\n            refCount,\n            maxBlockSize: 1 << 16,\n            firstDataLine,\n            csiVersion,\n            indices,\n            depth: this.depth,\n            maxBinNumber: this.maxBinNumber,\n            maxRefLength,\n        };\n    }\n    parsePseudoBin(bytes, offset) {\n        const lineCount = longToNumber(Long.fromBytesLE(Array.prototype.slice.call(bytes, offset + 28, offset + 36), true));\n        return { lineCount };\n    }\n    async blocksForRange(refName, min, max, opts = {}) {\n        if (min < 0) {\n            min = 0;\n        }\n        const indexData = await this.parse(opts);\n        if (!indexData) {\n            return [];\n        }\n        const refId = indexData.refNameToId[refName];\n        const ba = indexData.indices[refId];\n        if (!ba) {\n            return [];\n        }\n        // const { linearIndex, binIndex } = indexes\n        const overlappingBins = this.reg2bins(min, max); // List of bin #s that overlap min, max\n        const chunks = [];\n        // Find chunks in overlapping bins.  Leaf bins (< 4681) are not pruned\n        for (const [start, end] of overlappingBins) {\n            for (let bin = start; bin <= end; bin++) {\n                if (ba.binIndex[bin]) {\n                    const binChunks = ba.binIndex[bin];\n                    for (let c = 0; c < binChunks.length; ++c) {\n                        chunks.push(new Chunk(binChunks[c].minv, binChunks[c].maxv, bin));\n                    }\n                }\n            }\n        }\n        return optimizeChunks(chunks, new VirtualOffset(0, 0));\n    }\n    /**\n     * calculate the list of bins that may overlap with region [beg,end) (zero-based half-open)\n     */\n    reg2bins(beg, end) {\n        beg -= 1; // < convert to 1-based closed\n        if (beg < 1) {\n            beg = 1;\n        }\n        if (end > 2 ** 50) {\n            end = 2 ** 34;\n        } // 17 GiB ought to be enough for anybody\n        end -= 1;\n        let l = 0;\n        let t = 0;\n        let s = this.minShift + this.depth * 3;\n        const bins = [];\n        for (; l <= this.depth; s -= 3, t += lshift(1, l * 3), l += 1) {\n            const b = t + rshift(beg, s);\n            const e = t + rshift(end, s);\n            if (e - b + bins.length > this.maxBinNumber) {\n                throw new Error(`query ${beg}-${end} is too large for current binning scheme (shift ${this.minShift}, depth ${this.depth}), try a smaller query or a coarser index binning scheme`);\n            }\n            bins.push([b, e]);\n        }\n        return bins;\n    }\n}\n//# sourceMappingURL=csi.js.map","import TabixIndexedFile from './tabixIndexedFile';\nimport TBI from './tbi';\nimport CSI from './csi';\nexport { TabixIndexedFile, TBI, CSI };\n//# sourceMappingURL=index.js.map","import AbortablePromiseCache from 'abortable-promise-cache';\nimport QuickLRU from 'quick-lru';\nexport default class IndexFile {\n    /**\n     * @param {filehandle} filehandle\n     * @param {function} [renameRefSeqs]\n     */\n    constructor({ filehandle, renameRefSeqs = (n) => n, }) {\n        this.filehandle = filehandle;\n        this.renameRefSeq = renameRefSeqs;\n    }\n    async getMetadata(opts = {}) {\n        // eslint-disable-next-line @typescript-eslint/no-unused-vars\n        const { indices, ...rest } = await this.parse(opts);\n        return rest;\n    }\n    _findFirstData(currentFdl, virtualOffset) {\n        if (currentFdl) {\n            return currentFdl.compareTo(virtualOffset) > 0\n                ? virtualOffset\n                : currentFdl;\n        }\n        else {\n            return virtualOffset;\n        }\n    }\n    async parse(opts = {}) {\n        if (!this._parseCache) {\n            this._parseCache = new AbortablePromiseCache({\n                cache: new QuickLRU({ maxSize: 1 }),\n                fill: () => this._parse(opts),\n            });\n        }\n        return this._parseCache.get('index', null, undefined);\n    }\n    async hasRefSeq(seqId, opts = {}) {\n        return !!((await this.parse(opts)).indices[seqId] || {}).binIndex;\n    }\n}\n//# sourceMappingURL=indexFile.js.map","import AbortablePromiseCache from 'abortable-promise-cache';\nimport LRU from 'quick-lru';\nimport { LocalFile } from 'generic-filehandle';\nimport { unzip, unzipChunkSlice } from '@gmod/bgzf-filehandle';\nimport { checkAbortSignal } from './util';\nimport TBI from './tbi';\nimport CSI from './csi';\nfunction timeout(time) {\n    return new Promise(resolve => {\n        setTimeout(resolve, time);\n    });\n}\nexport default class TabixIndexedFile {\n    /**\n     * @param {object} args\n     * @param {string} [args.path]\n     * @param {filehandle} [args.filehandle]\n     * @param {string} [args.tbiPath]\n     * @param {filehandle} [args.tbiFilehandle]\n     * @param {string} [args.csiPath]\n     * @param {filehandle} [args.csiFilehandle]\n     * @param {chunkSizeLimit} default 50MiB\n     * @param {function} [args.renameRefSeqs] optional function with sig `string => string` to transform\n     * reference sequence names for the purpose of indexing and querying. note that the data that is returned is\n     * not altered, just the names of the reference sequences that are used for querying.\n     * @param {number} [args.chunkCacheSize] maximum size in bytes of the chunk cache. default 5MB\n     * @param {number} [args.blockCacheSize] maximum size in bytes of the block cache. default 5MB\n     */\n    constructor({ path, filehandle, tbiPath, tbiFilehandle, csiPath, csiFilehandle, chunkSizeLimit = 50000000, renameRefSeqs = n => n, chunkCacheSize = 5 * 2 ** 20, }) {\n        if (filehandle) {\n            this.filehandle = filehandle;\n        }\n        else if (path) {\n            this.filehandle = new LocalFile(path);\n        }\n        else {\n            throw new TypeError('must provide either filehandle or path');\n        }\n        if (tbiFilehandle) {\n            this.index = new TBI({\n                filehandle: tbiFilehandle,\n                renameRefSeqs,\n            });\n        }\n        else if (csiFilehandle) {\n            this.index = new CSI({\n                filehandle: csiFilehandle,\n                renameRefSeqs,\n            });\n        }\n        else if (tbiPath) {\n            this.index = new TBI({\n                filehandle: new LocalFile(tbiPath),\n                renameRefSeqs,\n            });\n        }\n        else if (csiPath) {\n            this.index = new CSI({\n                filehandle: new LocalFile(csiPath),\n                renameRefSeqs,\n            });\n        }\n        else if (path) {\n            this.index = new TBI({\n                filehandle: new LocalFile(`${path}.tbi`),\n                renameRefSeqs,\n            });\n        }\n        else {\n            throw new TypeError('must provide one of tbiFilehandle, tbiPath, csiFilehandle, or csiPath');\n        }\n        this.chunkSizeLimit = chunkSizeLimit;\n        this.renameRefSeq = renameRefSeqs;\n        this.chunkCache = new AbortablePromiseCache({\n            cache: new LRU({\n                maxSize: Math.floor(chunkCacheSize / (1 << 16)),\n            }),\n            fill: this.readChunk.bind(this),\n        });\n    }\n    /**\n     * @param {string} refName name of the reference sequence\n     * @param {number} start start of the region (in 0-based half-open coordinates)\n     * @param {number} end end of the region (in 0-based half-open coordinates)\n     * @param {function|object} lineCallback callback called for each line in the region. can also pass a object param containing obj.lineCallback, obj.signal, etc\n     * @returns {Promise} resolved when the whole read is finished, rejected on error\n     */\n    async getLines(refName, start, end, opts) {\n        let signal;\n        let options = {};\n        let callback;\n        if (typeof opts === 'undefined') {\n            throw new TypeError('line callback must be provided');\n        }\n        if (typeof opts === 'function') {\n            callback = opts;\n        }\n        else {\n            options = opts;\n            callback = opts.lineCallback;\n        }\n        if (refName === undefined) {\n            throw new TypeError('must provide a reference sequence name');\n        }\n        if (!callback) {\n            throw new TypeError('line callback must be provided');\n        }\n        const metadata = await this.index.getMetadata(options);\n        checkAbortSignal(signal);\n        if (!start) {\n            start = 0;\n        }\n        if (!end) {\n            end = metadata.maxRefLength;\n        }\n        if (!(start <= end)) {\n            throw new TypeError('invalid start and end coordinates. start must be less than or equal to end');\n        }\n        if (start === end) {\n            return;\n        }\n        const chunks = await this.index.blocksForRange(refName, start, end, options);\n        checkAbortSignal(signal);\n        // check the chunks for any that are over the size limit.  if\n        // any are, don't fetch any of them\n        for (let i = 0; i < chunks.length; i += 1) {\n            const size = chunks[i].fetchedSize();\n            if (size > this.chunkSizeLimit) {\n                throw new Error(`Too much data. Chunk size ${size.toLocaleString()} bytes exceeds chunkSizeLimit of ${this.chunkSizeLimit.toLocaleString()}.`);\n            }\n        }\n        // now go through each chunk and parse and filter the lines out of it\n        let last = Date.now();\n        for (let chunkNum = 0; chunkNum < chunks.length; chunkNum += 1) {\n            let previousStartCoordinate;\n            const c = chunks[chunkNum];\n            const { buffer, cpositions, dpositions } = await this.chunkCache.get(c.toString(), c, signal);\n            const lines = (typeof TextDecoder !== 'undefined'\n                ? new TextDecoder('utf-8').decode(buffer)\n                : buffer.toString()).split('\\n');\n            lines.pop();\n            checkAbortSignal(signal);\n            let blockStart = c.minv.dataPosition;\n            let pos;\n            for (let i = 0; i < lines.length; i += 1) {\n                const line = lines[i];\n                for (pos = 0; blockStart >= dpositions[pos]; pos += 1) { }\n                // filter the line for whether it is within the requested range\n                const { startCoordinate, overlaps } = this.checkLine(metadata, refName, start, end, line);\n                // do a small check just to make sure that the lines are really sorted by start coordinate\n                if (previousStartCoordinate !== undefined &&\n                    startCoordinate !== undefined &&\n                    previousStartCoordinate > startCoordinate) {\n                    throw new Error(`Lines not sorted by start coordinate (${previousStartCoordinate} > ${startCoordinate}), this file is not usable with Tabix.`);\n                }\n                previousStartCoordinate = startCoordinate;\n                if (overlaps) {\n                    callback(line.trim(), \n                    // cpositions[pos] refers to actual file offset of a bgzip block boundaries\n                    //\n                    // we multiply by (1 <<8) in order to make sure each block has a \"unique\"\n                    // address space so that data in that block could never overlap\n                    //\n                    // then the blockStart-dpositions is an uncompressed file offset from\n                    // that bgzip block boundary, and since the cpositions are multiplied by\n                    // (1 << 8) these uncompressed offsets get a unique space\n                    cpositions[pos] * (1 << 8) + (blockStart - dpositions[pos]));\n                }\n                else if (startCoordinate !== undefined && startCoordinate >= end) {\n                    // the lines were overlapping the region, but now have stopped, so\n                    // we must be at the end of the relevant data and we can stop\n                    // processing data now\n                    return;\n                }\n                blockStart += line.length + 1;\n                // yield if we have emitted beyond the yield limit\n                if (last - Date.now() > 500) {\n                    last = Date.now();\n                    checkAbortSignal(signal);\n                    await timeout(1);\n                }\n            }\n        }\n    }\n    async getMetadata(opts = {}) {\n        return this.index.getMetadata(opts);\n    }\n    /**\n     * get a buffer containing the \"header\" region of\n     * the file, which are the bytes up to the first\n     * non-meta line\n     *\n     * @returns {Promise} for a buffer\n     */\n    async getHeaderBuffer(opts = {}) {\n        const { firstDataLine, metaChar, maxBlockSize } = await this.getMetadata(opts);\n        checkAbortSignal(opts.signal);\n        const maxFetch = firstDataLine && firstDataLine.blockPosition\n            ? firstDataLine.blockPosition + maxBlockSize\n            : maxBlockSize;\n        // TODO: what if we don't have a firstDataLine, and the header\n        // actually takes up more than one block? this case is not covered here\n        let bytes = await this._readRegion(0, maxFetch, opts);\n        checkAbortSignal(opts.signal);\n        try {\n            bytes = await unzip(bytes);\n        }\n        catch (e) {\n            console.error(e);\n            throw new Error(\n            //@ts-ignore\n            `error decompressing block ${e.code} at 0 (length ${maxFetch}) ${e}`);\n        }\n        // trim off lines after the last non-meta line\n        if (metaChar) {\n            // trim backward from the end\n            let lastNewline = -1;\n            const newlineByte = '\\n'.charCodeAt(0);\n            const metaByte = metaChar.charCodeAt(0);\n            for (let i = 0; i < bytes.length; i += 1) {\n                if (i === lastNewline + 1 && bytes[i] !== metaByte) {\n                    break;\n                }\n                if (bytes[i] === newlineByte) {\n                    lastNewline = i;\n                }\n            }\n            bytes = bytes.slice(0, lastNewline + 1);\n        }\n        return bytes;\n    }\n    /**\n     * get a string containing the \"header\" region of the\n     * file, is the portion up to the first non-meta line\n     *\n     * @returns {Promise} for a string\n     */\n    async getHeader(opts = {}) {\n        const bytes = await this.getHeaderBuffer(opts);\n        checkAbortSignal(opts.signal);\n        return bytes.toString('utf8');\n    }\n    /**\n     * get an array of reference sequence names, in the order in which\n     * they occur in the file.\n     *\n     * reference sequence renaming is not applied to these names.\n     *\n     * @returns {Promise} for an array of string sequence names\n     */\n    async getReferenceSequenceNames(opts = {}) {\n        const metadata = await this.getMetadata(opts);\n        return metadata.refIdToName;\n    }\n    /**\n     * @param {object} metadata metadata object from the parsed index,\n     * containing columnNumbers, metaChar, and format\n     * @param {string} regionRefName\n     * @param {number} regionStart region start coordinate (0-based-half-open)\n     * @param {number} regionEnd region end coordinate (0-based-half-open)\n     * @param {array[string]} line\n     * @returns {object} like `{startCoordinate, overlaps}`. overlaps is boolean,\n     * true if line is a data line that overlaps the given region\n     */\n    checkLine({ columnNumbers, metaChar, coordinateType, format, }, regionRefName, regionStart, regionEnd, line) {\n        // skip meta lines\n        if (line.charAt(0) === metaChar) {\n            return { overlaps: false };\n        }\n        // check ref/start/end using column metadata from index\n        let { ref, start, end } = columnNumbers;\n        if (!ref) {\n            ref = 0;\n        }\n        if (!start) {\n            start = 0;\n        }\n        if (!end) {\n            end = 0;\n        }\n        if (format === 'VCF') {\n            end = 8;\n        }\n        const maxColumn = Math.max(ref, start, end);\n        // this code is kind of complex, but it is fairly fast.\n        // basically, we want to avoid doing a split, because if the lines are really long\n        // that could lead to us allocating a bunch of extra memory, which is slow\n        let currentColumnNumber = 1; // cols are numbered starting at 1 in the index metadata\n        let currentColumnStart = 0;\n        let refSeq = '';\n        let startCoordinate = -Infinity;\n        for (let i = 0; i < line.length + 1; i += 1) {\n            if (line[i] === '\\t' || i === line.length) {\n                if (currentColumnNumber === ref) {\n                    if (this.renameRefSeq(line.slice(currentColumnStart, i)) !==\n                        regionRefName) {\n                        return { overlaps: false };\n                    }\n                }\n                else if (currentColumnNumber === start) {\n                    startCoordinate = parseInt(line.slice(currentColumnStart, i), 10);\n                    // we convert to 0-based-half-open\n                    if (coordinateType === '1-based-closed') {\n                        startCoordinate -= 1;\n                    }\n                    if (startCoordinate >= regionEnd) {\n                        return { startCoordinate, overlaps: false };\n                    }\n                    if (end === 0 || end === start) {\n                        // if we have no end, we assume the feature is 1 bp long\n                        if (startCoordinate + 1 <= regionStart) {\n                            return { startCoordinate, overlaps: false };\n                        }\n                    }\n                }\n                else if (format === 'VCF' && currentColumnNumber === 4) {\n                    refSeq = line.slice(currentColumnStart, i);\n                }\n                else if (currentColumnNumber === end) {\n                    let endCoordinate;\n                    // this will never match if there is no end column\n                    if (format === 'VCF') {\n                        endCoordinate = this._getVcfEnd(startCoordinate, refSeq, line.slice(currentColumnStart, i));\n                    }\n                    else {\n                        endCoordinate = parseInt(line.slice(currentColumnStart, i), 10);\n                    }\n                    if (endCoordinate <= regionStart) {\n                        return { overlaps: false };\n                    }\n                }\n                currentColumnStart = i + 1;\n                currentColumnNumber += 1;\n                if (currentColumnNumber > maxColumn) {\n                    break;\n                }\n            }\n        }\n        return { startCoordinate, overlaps: true };\n    }\n    _getVcfEnd(startCoordinate, refSeq, info) {\n        let endCoordinate = startCoordinate + refSeq.length;\n        // ignore TRA features as they specify CHR2 and END\n        // as being on a different chromosome\n        // if CHR2 is on the same chromosome, still ignore it\n        // because there should be another pairwise feature\n        // at the end of this one\n        const isTRA = info.indexOf('SVTYPE=TRA') !== -1;\n        if (info[0] !== '.' && !isTRA) {\n            let prevChar = ';';\n            for (let j = 0; j < info.length; j += 1) {\n                if (prevChar === ';' && info.slice(j, j + 4) === 'END=') {\n                    let valueEnd = info.indexOf(';', j);\n                    if (valueEnd === -1) {\n                        valueEnd = info.length;\n                    }\n                    endCoordinate = parseInt(info.slice(j + 4, valueEnd), 10);\n                    break;\n                }\n                prevChar = info[j];\n            }\n        }\n        else if (isTRA) {\n            return startCoordinate + 1;\n        }\n        return endCoordinate;\n    }\n    /**\n     * return the approximate number of data lines in the given reference sequence\n     * @param {string} refSeq reference sequence name\n     * @returns {Promise} for number of data lines present on that reference sequence\n     */\n    async lineCount(refName, opts = {}) {\n        return this.index.lineCount(refName, opts);\n    }\n    async _readRegion(position, compressedSize, opts = {}) {\n        const { bytesRead, buffer } = await this.filehandle.read(Buffer.alloc(compressedSize), 0, compressedSize, position, opts);\n        return bytesRead < compressedSize ? buffer.slice(0, bytesRead) : buffer;\n    }\n    /**\n     * read and uncompress the data in a chunk (composed of one or more\n     * contiguous bgzip blocks) of the file\n     * @param {Chunk} chunk\n     * @returns {Promise} for a string chunk of the file\n     */\n    async readChunk(chunk, opts = {}) {\n        // fetch the uncompressed data, uncompress carefully a block at a time,\n        // and stop when done\n        const compressedData = await this._readRegion(chunk.minv.blockPosition, chunk.fetchedSize(), opts);\n        try {\n            return unzipChunkSlice(compressedData, chunk);\n        }\n        catch (e) {\n            throw new Error(`error decompressing chunk ${chunk.toString()} ${e}`);\n        }\n    }\n}\n//# sourceMappingURL=tabixIndexedFile.js.map","import Long from 'long';\nimport VirtualOffset, { fromBytes } from './virtualOffset';\nimport Chunk from './chunk';\nimport { unzip } from '@gmod/bgzf-filehandle';\nimport { longToNumber, optimizeChunks, checkAbortSignal } from './util';\nimport IndexFile from './indexFile';\nconst TBI_MAGIC = 21578324; // TBI\\1\nconst TAD_LIDX_SHIFT = 14;\n/**\n * calculate the list of bins that may overlap with region [beg,end) (zero-based half-open)\n */\nfunction reg2bins(beg, end) {\n    beg += 1; // < convert to 1-based closed\n    end -= 1;\n    return [\n        [0, 0],\n        [1 + (beg >> 26), 1 + (end >> 26)],\n        [9 + (beg >> 23), 9 + (end >> 23)],\n        [73 + (beg >> 20), 73 + (end >> 20)],\n        [585 + (beg >> 17), 585 + (end >> 17)],\n        [4681 + (beg >> 14), 4681 + (end >> 14)],\n    ];\n}\nexport default class TabixIndex extends IndexFile {\n    async lineCount(refName, opts = {}) {\n        const indexData = await this.parse(opts);\n        if (!indexData) {\n            return -1;\n        }\n        const refId = indexData.refNameToId[refName];\n        const idx = indexData.indices[refId];\n        if (!idx) {\n            return -1;\n        }\n        const { stats } = indexData.indices[refId];\n        if (stats) {\n            return stats.lineCount;\n        }\n        return -1;\n    }\n    // memoize\n    // fetch and parse the index\n    async _parse(opts = {}) {\n        const bytes = await unzip((await this.filehandle.readFile(opts)));\n        checkAbortSignal(opts.signal);\n        // check TBI magic numbers\n        if (bytes.readUInt32LE(0) !== TBI_MAGIC /* \"TBI\\1\" */) {\n            throw new Error('Not a TBI file');\n            // TODO: do we need to support big-endian TBI files?\n        }\n        // number of reference sequences in the index\n        const refCount = bytes.readInt32LE(4);\n        const formatFlags = bytes.readInt32LE(8);\n        const coordinateType = formatFlags & 0x10000 ? 'zero-based-half-open' : '1-based-closed';\n        const formatOpts = {\n            0: 'generic',\n            1: 'SAM',\n            2: 'VCF',\n        };\n        const format = formatOpts[formatFlags & 0xf];\n        if (!format) {\n            throw new Error(`invalid Tabix preset format flags ${formatFlags}`);\n        }\n        const columnNumbers = {\n            ref: bytes.readInt32LE(12),\n            start: bytes.readInt32LE(16),\n            end: bytes.readInt32LE(20),\n        };\n        const metaValue = bytes.readInt32LE(24);\n        const depth = 5;\n        const maxBinNumber = ((1 << ((depth + 1) * 3)) - 1) / 7;\n        const maxRefLength = 2 ** (14 + depth * 3);\n        const metaChar = metaValue ? String.fromCharCode(metaValue) : null;\n        const skipLines = bytes.readInt32LE(28);\n        // read sequence dictionary\n        const nameSectionLength = bytes.readInt32LE(32);\n        const { refNameToId, refIdToName } = this._parseNameBytes(bytes.slice(36, 36 + nameSectionLength));\n        // read the indexes for each reference sequence\n        let currOffset = 36 + nameSectionLength;\n        let firstDataLine;\n        const indices = new Array(refCount).fill(0).map(() => {\n            // the binning index\n            const binCount = bytes.readInt32LE(currOffset);\n            currOffset += 4;\n            const binIndex = {};\n            let stats;\n            for (let j = 0; j < binCount; j += 1) {\n                const bin = bytes.readUInt32LE(currOffset);\n                currOffset += 4;\n                if (bin > maxBinNumber + 1) {\n                    throw new Error('tabix index contains too many bins, please use a CSI index');\n                }\n                else if (bin === maxBinNumber + 1) {\n                    const chunkCount = bytes.readInt32LE(currOffset);\n                    currOffset += 4;\n                    if (chunkCount === 2) {\n                        stats = this.parsePseudoBin(bytes, currOffset);\n                    }\n                    currOffset += 16 * chunkCount;\n                }\n                else {\n                    const chunkCount = bytes.readInt32LE(currOffset);\n                    currOffset += 4;\n                    const chunks = new Array(chunkCount);\n                    for (let k = 0; k < chunkCount; k += 1) {\n                        const u = fromBytes(bytes, currOffset);\n                        const v = fromBytes(bytes, currOffset + 8);\n                        currOffset += 16;\n                        firstDataLine = this._findFirstData(firstDataLine, u);\n                        chunks[k] = new Chunk(u, v, bin);\n                    }\n                    binIndex[bin] = chunks;\n                }\n            }\n            // the linear index\n            const linearCount = bytes.readInt32LE(currOffset);\n            currOffset += 4;\n            const linearIndex = new Array(linearCount);\n            for (let k = 0; k < linearCount; k += 1) {\n                linearIndex[k] = fromBytes(bytes, currOffset);\n                currOffset += 8;\n                firstDataLine = this._findFirstData(firstDataLine, linearIndex[k]);\n            }\n            return { binIndex, linearIndex, stats };\n        });\n        return {\n            indices,\n            metaChar,\n            maxBinNumber,\n            maxRefLength,\n            skipLines,\n            firstDataLine,\n            columnNumbers,\n            coordinateType,\n            format,\n            refIdToName,\n            refNameToId,\n            maxBlockSize: 1 << 16,\n        };\n    }\n    parsePseudoBin(bytes, offset) {\n        const lineCount = longToNumber(Long.fromBytesLE(bytes.slice(offset + 16, offset + 24), true));\n        return { lineCount };\n    }\n    _parseNameBytes(namesBytes) {\n        let currRefId = 0;\n        let currNameStart = 0;\n        const refIdToName = [];\n        const refNameToId = {};\n        for (let i = 0; i < namesBytes.length; i += 1) {\n            if (!namesBytes[i]) {\n                if (currNameStart < i) {\n                    let refName = namesBytes.toString('utf8', currNameStart, i);\n                    refName = this.renameRefSeq(refName);\n                    refIdToName[currRefId] = refName;\n                    refNameToId[refName] = currRefId;\n                }\n                currNameStart = i + 1;\n                currRefId += 1;\n            }\n        }\n        return { refNameToId, refIdToName };\n    }\n    async blocksForRange(refName, min, max, opts = {}) {\n        if (min < 0) {\n            min = 0;\n        }\n        const indexData = await this.parse(opts);\n        if (!indexData) {\n            return [];\n        }\n        const refId = indexData.refNameToId[refName];\n        const ba = indexData.indices[refId];\n        if (!ba) {\n            return [];\n        }\n        const minOffset = ba.linearIndex.length\n            ? ba.linearIndex[min >> TAD_LIDX_SHIFT >= ba.linearIndex.length\n                ? ba.linearIndex.length - 1\n                : min >> TAD_LIDX_SHIFT]\n            : new VirtualOffset(0, 0);\n        if (!minOffset) {\n            console.warn('querying outside of possible tabix range');\n        }\n        // const { linearIndex, binIndex } = indexes\n        const overlappingBins = reg2bins(min, max); // List of bin #s that overlap min, max\n        const chunks = [];\n        // Find chunks in overlapping bins.  Leaf bins (< 4681) are not pruned\n        for (const [start, end] of overlappingBins) {\n            for (let bin = start; bin <= end; bin++) {\n                if (ba.binIndex[bin]) {\n                    const binChunks = ba.binIndex[bin];\n                    for (let c = 0; c < binChunks.length; ++c) {\n                        chunks.push(new Chunk(binChunks[c].minv, binChunks[c].maxv, bin));\n                    }\n                }\n            }\n        }\n        // Use the linear index to find minimum file position of chunks that could\n        // contain alignments in the region\n        const nintv = ba.linearIndex.length;\n        let lowest = null;\n        const minLin = Math.min(min >> 14, nintv - 1);\n        const maxLin = Math.min(max >> 14, nintv - 1);\n        for (let i = minLin; i <= maxLin; ++i) {\n            const vp = ba.linearIndex[i];\n            if (vp) {\n                if (!lowest || vp.compareTo(lowest) < 0) {\n                    lowest = vp;\n                }\n            }\n        }\n        return optimizeChunks(chunks, lowest);\n    }\n}\n//# sourceMappingURL=tbi.js.map","export function longToNumber(long) {\n    if (long.greaterThan(Number.MAX_SAFE_INTEGER) ||\n        long.lessThan(Number.MIN_SAFE_INTEGER)) {\n        throw new Error('integer overflow');\n    }\n    return long.toNumber();\n}\nclass AbortError extends Error {\n}\n/**\n * Properly check if the given AbortSignal is aborted.\n * Per the standard, if the signal reads as aborted,\n * this function throws either a DOMException AbortError, or a regular error\n * with a `code` attribute set to `ERR_ABORTED`.\n *\n * For convenience, passing `undefined` is a no-op\n *\n * @param {AbortSignal} [signal] an AbortSignal, or anything with an `aborted` attribute\n * @returns nothing\n */\nexport function checkAbortSignal(signal) {\n    if (!signal) {\n        return;\n    }\n    if (signal.aborted) {\n        // console.log('bam aborted!')\n        if (typeof DOMException !== 'undefined') {\n            // eslint-disable-next-line  no-undef\n            throw new DOMException('aborted', 'AbortError');\n        }\n        else {\n            const e = new AbortError('aborted');\n            e.code = 'ERR_ABORTED';\n            throw e;\n        }\n    }\n}\n/**\n * Skips to the next tick, then runs `checkAbortSignal`.\n * Await this to inside an otherwise synchronous loop to\n * provide a place to break when an abort signal is received.\n * @param {AbortSignal} signal\n */\nexport async function abortBreakPoint(signal) {\n    await Promise.resolve();\n    checkAbortSignal(signal);\n}\nexport function canMergeBlocks(chunk1, chunk2) {\n    return (chunk2.minv.blockPosition - chunk1.maxv.blockPosition < 65000 &&\n        chunk2.maxv.blockPosition - chunk1.minv.blockPosition < 5000000);\n}\nexport function optimizeChunks(chunks, lowest) {\n    const mergedChunks = [];\n    let lastChunk = null;\n    if (chunks.length === 0) {\n        return chunks;\n    }\n    chunks.sort(function (c0, c1) {\n        const dif = c0.minv.blockPosition - c1.minv.blockPosition;\n        if (dif !== 0) {\n            return dif;\n        }\n        else {\n            return c0.minv.dataPosition - c1.minv.dataPosition;\n        }\n    });\n    chunks.forEach(chunk => {\n        if (!lowest || chunk.maxv.compareTo(lowest) > 0) {\n            if (lastChunk === null) {\n                mergedChunks.push(chunk);\n                lastChunk = chunk;\n            }\n            else {\n                if (canMergeBlocks(lastChunk, chunk)) {\n                    if (chunk.maxv.compareTo(lastChunk.maxv) > 0) {\n                        lastChunk.maxv = chunk.maxv;\n                    }\n                }\n                else {\n                    mergedChunks.push(chunk);\n                    lastChunk = chunk;\n                }\n            }\n        }\n        // else {\n        //   console.log(`skipping chunk ${chunk}`)\n        // }\n    });\n    return mergedChunks;\n}\n//# sourceMappingURL=util.js.map","export default class VirtualOffset {\n    constructor(blockPosition, dataPosition) {\n        this.blockPosition = blockPosition; // < offset of the compressed data block\n        this.dataPosition = dataPosition; // < offset into the uncompressed data\n    }\n    toString() {\n        return `${this.blockPosition}:${this.dataPosition}`;\n    }\n    compareTo(b) {\n        return (this.blockPosition - b.blockPosition || this.dataPosition - b.dataPosition);\n    }\n    static min(...args) {\n        let min;\n        let i = 0;\n        for (; !min; i += 1) {\n            min = args[i];\n        }\n        for (; i < args.length; i += 1) {\n            if (min.compareTo(args[i]) > 0) {\n                min = args[i];\n            }\n        }\n        return min;\n    }\n}\nexport function fromBytes(bytes, offset = 0, bigendian = false) {\n    if (bigendian) {\n        throw new Error('big-endian virtual file offsets not implemented');\n    }\n    return new VirtualOffset(bytes[offset + 7] * 0x10000000000 +\n        bytes[offset + 6] * 0x100000000 +\n        bytes[offset + 5] * 0x1000000 +\n        bytes[offset + 4] * 0x10000 +\n        bytes[offset + 3] * 0x100 +\n        bytes[offset + 2], (bytes[offset + 1] << 8) | bytes[offset]);\n}\n//# sourceMappingURL=virtualOffset.js.map","'use strict';\n\nclass QuickLRU {\n\tconstructor(options = {}) {\n\t\tif (!(options.maxSize && options.maxSize > 0)) {\n\t\t\tthrow new TypeError('`maxSize` must be a number greater than 0');\n\t\t}\n\n\t\tthis.maxSize = options.maxSize;\n\t\tthis.cache = new Map();\n\t\tthis.oldCache = new Map();\n\t\tthis._size = 0;\n\t}\n\n\t_set(key, value) {\n\t\tthis.cache.set(key, value);\n\t\tthis._size++;\n\n\t\tif (this._size >= this.maxSize) {\n\t\t\tthis._size = 0;\n\t\t\tthis.oldCache = this.cache;\n\t\t\tthis.cache = new Map();\n\t\t}\n\t}\n\n\tget(key) {\n\t\tif (this.cache.has(key)) {\n\t\t\treturn this.cache.get(key);\n\t\t}\n\n\t\tif (this.oldCache.has(key)) {\n\t\t\tconst value = this.oldCache.get(key);\n\t\t\tthis.oldCache.delete(key);\n\t\t\tthis._set(key, value);\n\t\t\treturn value;\n\t\t}\n\t}\n\n\tset(key, value) {\n\t\tif (this.cache.has(key)) {\n\t\t\tthis.cache.set(key, value);\n\t\t} else {\n\t\t\tthis._set(key, value);\n\t\t}\n\n\t\treturn this;\n\t}\n\n\thas(key) {\n\t\treturn this.cache.has(key) || this.oldCache.has(key);\n\t}\n\n\tpeek(key) {\n\t\tif (this.cache.has(key)) {\n\t\t\treturn this.cache.get(key);\n\t\t}\n\n\t\tif (this.oldCache.has(key)) {\n\t\t\treturn this.oldCache.get(key);\n\t\t}\n\t}\n\n\tdelete(key) {\n\t\tconst deleted = this.cache.delete(key);\n\t\tif (deleted) {\n\t\t\tthis._size--;\n\t\t}\n\n\t\treturn this.oldCache.delete(key) || deleted;\n\t}\n\n\tclear() {\n\t\tthis.cache.clear();\n\t\tthis.oldCache.clear();\n\t\tthis._size = 0;\n\t}\n\n\t* keys() {\n\t\tfor (const [key] of this) {\n\t\t\tyield key;\n\t\t}\n\t}\n\n\t* values() {\n\t\tfor (const [, value] of this) {\n\t\t\tyield value;\n\t\t}\n\t}\n\n\t* [Symbol.iterator]() {\n\t\tfor (const item of this.cache) {\n\t\t\tyield item;\n\t\t}\n\n\t\tfor (const item of this.oldCache) {\n\t\t\tconst [key] = item;\n\t\t\tif (!this.cache.has(key)) {\n\t\t\t\tyield item;\n\t\t\t}\n\t\t}\n\t}\n\n\tget size() {\n\t\tlet oldCacheSize = 0;\n\t\tfor (const key of this.oldCache.keys()) {\n\t\t\tif (!this.cache.has(key)) {\n\t\t\t\toldCacheSize++;\n\t\t\t}\n\t\t}\n\n\t\treturn this._size + oldCacheSize;\n\t}\n}\n\nmodule.exports = QuickLRU;\n"],"names":[],"sourceRoot":""}